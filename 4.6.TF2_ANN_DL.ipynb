{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN & Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron(퍼셉트론)\n",
    "1958년 Frank Rosenblatt(프랭크 로젠블랫)\n",
    "생물학적 뇌의 뉴런을 모방하여 만든 인공신경망의 기본 단위\n",
    "여러 입력을 값으로 받아 하나의 0 또는 1을 출력하는 함수\n",
    "뇌의 신경세포 즉, 뉴런과 같은 역할\n",
    "![image.png](https://i.imgur.com/Ufrgh5x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron\n",
    "* Single-Layer Perceptron\n",
    "    * 하나의 Perceptron 만을 이용\n",
    "    * Logistic Regression\n",
    "* Multi-Layer Perceptron\n",
    "    * 여러개의 퍼셉트론 이용\n",
    "    * Hidden-Layer\n",
    "        * 정확한 상태를 알 수 없다.\n",
    "\n",
    "![image.png](https://i.imgur.com/ujilihQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network\n",
    "* 인공 신경망\n",
    "* Perceptron을 여러개 연결한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron의 필요성\n",
    "* 비 선형 문제 \n",
    "* 대표적 사례 : XOR 문제\n",
    "* 고차원 다항식\n",
    "![image.png](https://i.imgur.com/YifwZKU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "* Deep Neural Network를 이용한 Machine Learning\n",
    "* Hidden-Layer가 2개 이상인 인공 신경망\n",
    "* 1000개 이상인 경우 도 일반적\n",
    "\n",
    "![image.png](https://i.imgur.com/7TYDnf3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상수 값으로 풀어보는 XOR  예제\n",
    "* 2개 층으로 구성된 NN으로 XOR 문제를 푸는 예시\n",
    "* 단, W와 b를 이미 알고 있는 값을 지정해서 푼다.\n",
    "* 이것이 가능하다면 여러 개층의 W와 b를 GD로 풀수 있으면 많은 문제를 딥러닝으로 해결이 가능하다는 말이다.\n",
    "* 여기서 사용한 W와 b\n",
    "![xor_nn_1](https://user-images.githubusercontent.com/661959/54298177-9e82f080-45fb-11e9-8bdd-1f86718c6f5d.png)\n",
    "* $W$와 $bias$\n",
    "    * Layer-1 y1: $W = \\begin{bmatrix} 5 \\\\5 \\end{bmatrix} , b= -8$\n",
    "    * Layer-1 y2: $W = \\begin{bmatrix}-7 \\\\ -7 \\end{bmatrix} , b= 3$\n",
    "    * Layer-2 : $W= \\begin{bmatrix}-11 \\\\ -11 \\end{bmatrix}, b = 6$\n",
    "* 연산식\n",
    "    * $x=(0,0)$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 = -8, sig(-8)=0$\n",
    "        * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = 3, sig(3)=1$\n",
    "            * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "    * $x=(0,1)$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}0 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,0)$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5 -8=-3, sig(-3)=0$\n",
    "        * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7+3, sig(-4)=0$\n",
    "            * $ \\begin{bmatrix}0 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =6, sig(6)=1$\n",
    "    * $x=(1,1)$\n",
    "        * $ \\begin{bmatrix}1 &1\\ \\end{bmatrix} \\begin{bmatrix}5\\\\5\\end{bmatrix}-8 =5+5-8=2, sig(2)=1$\n",
    "        * $ \\begin{bmatrix}1 &1\\ \\end{bmatrix} \\begin{bmatrix}-7\\\\-7\\end{bmatrix}+3 = -7-7+3=-11, sig(-11)=0$\n",
    "            * $ \\begin{bmatrix}1 &0\\ \\end{bmatrix} \\begin{bmatrix}-11\\\\-11\\end{bmatrix}+6 =-11+6= -5, sig(-5)=0$\n",
    "* 연산 결과\n",
    "\n",
    "| $x_1$ $x_2$ | $y_1$ $y_2$ | $\\hat y$|\n",
    "|---|---|---|\n",
    "|0,0|0,1|0\n",
    "|0,1|0,0|1\n",
    "|1,0|0,0|1\n",
    "|1,1|1,0|0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow를 이용한 XOR 문제 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR 문제 Multi-variable 형식으로 축소\n",
    "![xor_nn_2](https://user-images.githubusercontent.com/661959/54298185-a347a480-45fb-11e9-91d4-e98111241794.png)\n",
    "\n",
    "* $W$와 bias\n",
    "    * $x = \\begin {bmatrix}0 & 0\\\\ 0&1\\\\1&0\\\\1&1 \\end {bmatrix}$\n",
    "    * $W_1 = \\begin{bmatrix}5&-7 \\\\ 5&-7\\end{bmatrix}$\n",
    "    * $b_1 = \\begin{bmatrix}-8 & 3 \\end{bmatrix}$\n",
    "    * $W_2 = \\begin{bmatrix}-11 \\\\ -11\\end{bmatrix}$\n",
    "    * $b_2 = 6 $\n",
    "* 연산식\n",
    "    * $ \\hat y =\n",
    "  \\begin{cases}\n",
    "    K(x) = sigmoid(Xw_1 + b_1)\\\\\n",
    "    H(x) = sigmoid(K(x)W_2 + b_2)\n",
    "  \\end{cases}\n",
    "    $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:\n",
      "[[0.01118419]\n",
      " [0.9949357 ]\n",
      " [0.9949357 ]\n",
      " [0.02438849]] \n",
      "Predicted:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(np.array([[5,-7], [5,-7]], dtype=np.float32), name='weight1')\n",
    "b1 = tf.Variable(np.array([[-8, 3]], dtype=np.float32), name='bias1')\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(np.array([[-11],[-11]], dtype=np.float32), name='weight2')\n",
    "b2 = tf.Variable(np.array([6], dtype=np.float32), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"\\nHypothesis:\\n{hypothesis} \\nPredicted:\\n{predicted} \\nAccuracy:\\n{accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* 역 전파 알고리즘\n",
    "* 출력층의 결과 오차를 입력층 까지 거슬러 전파하면서 계산\n",
    "* 미분 연산\n",
    "    * ReLu 활성화 함수  \n",
    "    * Sigmoid 미분 불가능\n",
    "![image.png](https://i.imgur.com/qdVRwxq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* Chain Rule\n",
    "    * $f(g(x))$, $f(g)$, $g(x)$에 대해 미분\n",
    "        * $\\displaystyle \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g}\\frac{\\partial g}{\\partial x}$\n",
    "![](https://i.imgur.com/uPVRjcD.png)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 문제 MLP 학습 예제\n",
    "* 앞서 상수로 풀었던 XOR 문제를 MLP 학습으로 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0,\t cost:0.843840479850769\n",
      "step:500,\t cost:0.6669301986694336\n",
      "step:1000,\t cost:0.6146426200866699\n",
      "step:1500,\t cost:0.48531442880630493\n",
      "step:2000,\t cost:0.26392289996147156\n",
      "step:2500,\t cost:0.14104312658309937\n",
      "step:3000,\t cost:0.08984574675559998\n",
      "step:3500,\t cost:0.06442225724458694\n",
      "step:4000,\t cost:0.04970189556479454\n",
      "step:4500,\t cost:0.04023773968219757\n",
      "step:5000,\t cost:0.03369186073541641\n",
      "step:5500,\t cost:0.028917381539940834\n",
      "step:6000,\t cost:0.025291990488767624\n",
      "step:6500,\t cost:0.02245151996612549\n",
      "step:7000,\t cost:0.020169492810964584\n",
      "step:7500,\t cost:0.01829811930656433\n",
      "step:8000,\t cost:0.01673700660467148\n",
      "step:8500,\t cost:0.015415857546031475\n",
      "step:9000,\t cost:0.014283902011811733\n",
      "step:9500,\t cost:0.013303708285093307\n",
      "step:10000,\t cost:0.012446886859834194\n",
      "Hypothesis:[[0.01204556]\n",
      " [0.9840926 ]\n",
      " [0.9887545 ]\n",
      " [0.01027125]] \n",
      "Predicted:[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:1.0\n",
      "w1:[[ 6.546847   5.758096 ]\n",
      " [-6.393722  -6.0447693]], b1:[ 3.2414317 -3.0711067], w2:[[-9.757918]\n",
      " [10.371084]], b2[4.524005]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random.normal([2]), name='bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random.normal([1]), name='bias2')\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        \n",
    "        d_W1, d_b1, d_W2, d_b2 = tape.gradient(cost, [W1, b1, W2, b2] )\n",
    "        \n",
    "        W1.assign_sub(learning_rate * d_W1)\n",
    "        b1.assign_sub(learning_rate * d_b1)\n",
    "        W2.assign_sub(learning_rate * d_W2)\n",
    "        b2.assign_sub(learning_rate * d_b2)\n",
    "        \n",
    "        \n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(f\"step:{step},\\t cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n",
    "print(f\"w1:{W1.numpy()}, b1:{b1.numpy()}, w2:{W2.numpy()}, b2{b2.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient\n",
    "![](https://i.imgur.com/kzQpyNL.png)\n",
    "* sigmoid 함수를 사용하면 input값들이(x1,x2,x3....xn) layer을 거치면서 0에 수렴\n",
    "* 0에 수렴하는 값들이 다른 layer의 input 값으로 입력된다.\n",
    "* 입력된 값들은 layer를 거치면서 0에 수렴\n",
    "* x1,x2,x3....xn의 값은 최종으로 출력 되는 값에 영향이 없다\n",
    "* cost가 줄어들지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:0.9359210729598999\n",
      "step:500, cost:0.6931285858154297\n",
      "step:1000, cost:0.6931250095367432\n",
      "step:1500, cost:0.6931213140487671\n",
      "step:2000, cost:0.6931175589561462\n",
      "step:2500, cost:0.6931136250495911\n",
      "step:3000, cost:0.6931094527244568\n",
      "step:3500, cost:0.6931052207946777\n",
      "step:4000, cost:0.6931006908416748\n",
      "step:4500, cost:0.6930959224700928\n",
      "step:5000, cost:0.6930908560752869\n",
      "step:5500, cost:0.6930854916572571\n",
      "step:6000, cost:0.6930798292160034\n",
      "step:6500, cost:0.6930736303329468\n",
      "step:7000, cost:0.6930670738220215\n",
      "step:7500, cost:0.6930599808692932\n",
      "step:8000, cost:0.6930522918701172\n",
      "step:8500, cost:0.6930439472198486\n",
      "step:9000, cost:0.693034827709198\n",
      "step:9500, cost:0.6930248737335205\n",
      "step:10000, cost:0.6930139064788818\n",
      "Hypothesis:[[0.4991413 ]\n",
      " [0.50023955]\n",
      " [0.49994606]\n",
      " [0.5007763 ]] \n",
      "Predicted:[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]] \n",
      "Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(tf.random.normal([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.sigmoid(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function \n",
    "* Rectified Linear Unit\n",
    "* `max(0,x)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "* 역 전파 알고리즘에 적합한 활성화 함수\n",
    "![image.png](https://i.imgur.com/OzBowJo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:1.5729037523269653\n",
      "step:1000, cost:0.001565066515468061\n",
      "step:2000, cost:0.0006272040773183107\n",
      "step:3000, cost:0.00037578874616883695\n",
      "step:4000, cost:0.00026327857631258667\n",
      "step:5000, cost:0.0002006246941164136\n",
      "step:6000, cost:0.00016095326282083988\n",
      "step:7000, cost:0.00013376263086684048\n",
      "step:8000, cost:0.00011402681411709636\n",
      "step:9000, cost:9.91808992694132e-05\n",
      "step:10000, cost:8.749539847485721e-05\n",
      "Hypothesis:[[1.1831522e-05]\n",
      " [9.9995661e-01]\n",
      " [9.9999398e-01]\n",
      " [2.8875470e-04]] \n",
      "Predicted:[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(tf.random.normal([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(tf.random.normal([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight 초기 값\n",
    "* Geffrey E. Hinton (2006) \" A Fast Learning Algorithm for Deep Belief Nets\"\n",
    "    * http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf\n",
    "    * 0을 사용하지 말것\n",
    "    * RBM(Restricted Boltzmann Machine) 으로 초기화\n",
    "        * 입력 값을 타겟으로 하는 W와 출력을 찾는 사전 훈련\n",
    "        * $  X \\times W = H $\n",
    "        * H와 W를 랜덤하게 초기화 해서 X와의 차이를 계산\n",
    "        * 입력된 X와 차이가 최소가 되는 W로 초기화\n",
    "    * 초창기에 사용\n",
    "* Xavier 초기화\n",
    "    * Xavier Glorot and Yoshua Bengio (2010)\n",
    "        * http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "    * 입력 값 갯수와 출력 값 갯수 사이의 난수를 입력 값 갯수의 제곱근으로 나눈다.\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in)`\n",
    "    * Tensorflow Initializer\n",
    "        * ```initializer= tf.initializers.GlorotUniform()\n",
    "             W = tf.Variable(itntializer(shape=[5,5]))\n",
    "        ```\n",
    "        * ```W = tf.get_variable('W1', shape=[5,5], initializer= tf.glorot_uniform_initializer())```\n",
    "* He 초기화\n",
    "    * Kaiming He (2015)\n",
    "        * https://arxiv.org/abs/1502.01852\n",
    "    * `W = np.random.randn(in, out)/np.sqrt(in/2)`\n",
    "    * 입력 값을 2로 나눈 제곱근, 분모가 작아지기 때문에 xavier 보다 넓은 범위의 난수 \n",
    "    * Tensorflow Initializer\n",
    "        * ```tf.initializers.he_normal()```\n",
    "    \n",
    "![](https://i.imgur.com/jrkciOO.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:1.1751172542572021\n",
      "step:1000, cost:0.000534312566742301\n",
      "step:2000, cost:0.00022219371749088168\n",
      "step:3000, cost:0.0001341819588560611\n",
      "step:4000, cost:9.413652878720313e-05\n",
      "step:5000, cost:7.16482027200982e-05\n",
      "step:6000, cost:5.735676677431911e-05\n",
      "step:7000, cost:4.755111876875162e-05\n",
      "step:8000, cost:4.051735595567152e-05\n",
      "step:9000, cost:3.515266871545464e-05\n",
      "step:10000, cost:3.095035572187044e-05\n",
      "Hypothesis:[[4.9054623e-05]\n",
      " [9.9998772e-01]\n",
      " [9.9998659e-01]\n",
      " [4.9054623e-05]] \n",
      "Predicted:[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.random.set_seed(777)  \n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "init = tf.initializers.GlorotUniform()\n",
    "W1 = tf.Variable(init([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(init([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(init([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(init([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(init([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(init([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "* Tensorflow Optimizers\n",
    "    * tf.keras.optimizers.SGD\n",
    "    * tf.keras.optimizers.Adam\n",
    "    * tf.keras.optimizers.Adagrad\n",
    "    * tf.keras.optimizers.RMSProp\n",
    "\n",
    "\n",
    "![image.png](https://i.imgur.com/JXzrIyO.png)    \n",
    "![image.png](https://cs231n.github.io/assets/nn3/opt1.gif)\n",
    "* Animation : http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, cost:0.7461910247802734\n",
      "step:1000, cost:0.5735446214675903\n",
      "step:2000, cost:0.3828366994857788\n",
      "step:3000, cost:0.15617062151432037\n",
      "step:4000, cost:0.052324190735816956\n",
      "step:5000, cost:0.018458422273397446\n",
      "step:6000, cost:0.0074576567858457565\n",
      "step:7000, cost:0.0032829998526722193\n",
      "step:8000, cost:0.0015288956928998232\n",
      "step:9000, cost:0.0007429352262988687\n",
      "step:10000, cost:0.00037358253030106425\n",
      "Hypothesis:[[3.3882260e-04]\n",
      " [9.9961030e-01]\n",
      " [9.9960715e-01]\n",
      " [3.7270784e-04]] \n",
      "Predicted:[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "Y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "init = tf.initializers.GlorotUniform()\n",
    "W1 = tf.Variable(init([2, 5]))\n",
    "b1 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W2 = tf.Variable(init([5, 5]))\n",
    "b2 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W3 = tf.Variable(init([5, 5]))\n",
    "b3 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W4 = tf.Variable(init([5, 5]))\n",
    "b4 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W5 = tf.Variable(init([5, 5]))\n",
    "b5 = tf.Variable(tf.random.normal([5]))\n",
    "\n",
    "W6 = tf.Variable(init([5, 1]))\n",
    "b6 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate=0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for step in range(10001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "        layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "        layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "        layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "        hypothesis = tf.sigmoid(tf.matmul(layer5, W6) + b6)\n",
    "\n",
    "        cost = -tf.reduce_mean(Y * tf.math.log(hypothesis) + (1 - Y) * tf.math.log(1 - hypothesis))\n",
    "        #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=hypothesis))\n",
    "        DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6 = \\\n",
    "                tape.gradient(cost, [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, W6, b6])\n",
    "        \n",
    "        optimizer.apply_gradients(grads_and_vars=zip([DW1, db1, DW2, db2, DW3, db3, DW4, db4,DW5, db5,DW6, db6],\\\n",
    "                                                     [W1, b1, W2, b2, W3, b3, W4, b4, W5, b5,  W6, b6]))\n",
    "        '''\n",
    "        W1.assign_sub(learning_rate * DW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W2.assign_sub(learning_rate * DW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        W3.assign_sub(learning_rate * DW3)\n",
    "        b3.assign_sub(learning_rate * db3)\n",
    "        W4.assign_sub(learning_rate * DW4)\n",
    "        b4.assign_sub(learning_rate * db4)\n",
    "        W5.assign_sub(learning_rate * DW5)\n",
    "        b5.assign_sub(learning_rate * db5)\n",
    "        W6.assign_sub(learning_rate * DW6)\n",
    "        b6.assign_sub(learning_rate * db6)\n",
    "        '''\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step:{step}, cost:{cost}\")\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "print(f\"Hypothesis:{hypothesis} \\nPredicted:{predicted} \\nAccuracy:{accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Nueral Net 을 이용한 붗꽃 분류\n",
    "* Dataset 출처 : https://www.openml.org/d/61\n",
    "* CSV 파일 읽기 : pandas.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepallength</th>\n",
       "      <th>sepalwidth</th>\n",
       "      <th>petallength</th>\n",
       "      <th>petalwidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepallength  sepalwidth  petallength  petalwidth            class\n",
       "0            5.1         3.5          1.4         0.2      Iris-setosa\n",
       "10           5.4         3.7          1.5         0.2      Iris-setosa\n",
       "20           5.4         3.4          1.7         0.2      Iris-setosa\n",
       "30           4.8         3.1          1.6         0.2      Iris-setosa\n",
       "40           5.0         3.5          1.3         0.3      Iris-setosa\n",
       "50           7.0         3.2          4.7         1.4  Iris-versicolor\n",
       "60           5.0         2.0          3.5         1.0  Iris-versicolor\n",
       "70           5.9         3.2          4.8         1.8  Iris-versicolor\n",
       "80           5.5         2.4          3.8         1.1  Iris-versicolor\n",
       "90           5.5         2.6          4.4         1.2  Iris-versicolor\n",
       "100          6.3         3.3          6.0         2.5   Iris-virginica\n",
       "110          6.5         3.2          5.1         2.0   Iris-virginica\n",
       "120          6.9         3.2          5.7         2.3   Iris-virginica\n",
       "130          7.4         2.8          6.1         1.9   Iris-virginica\n",
       "140          6.7         3.1          5.6         2.4   Iris-virginica"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "csv.iloc[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 데이타 선행 처리\n",
    "* Category to Number :```class, index = np.unique(y, return_inverse=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']])\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "label, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 데이타 선행처리2\n",
    "* 뒤섞기 : ```np.random.shuffle()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1,\n",
       "        1, 2, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 1,\n",
       "        2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 2, 2, 1, 0, 0, 2, 1, 2, 2, 1, 1, 1,\n",
       "        2, 2, 2, 2, 0, 1, 2, 1, 2, 0, 2, 1, 1, 0, 2, 2, 0, 1, 1, 2, 2, 0,\n",
       "        1, 1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 2, 2, 1, 1, 0,\n",
       "        2, 2, 0, 1, 1, 2, 2, 2, 0, 1, 0, 1, 1, 2, 0, 2, 2, 1, 0, 2, 1, 2,\n",
       "        1, 0, 1, 2, 0, 0, 0, 2, 1, 0, 2, 1, 1, 1, 1, 2, 1, 0], dtype=int64),\n",
       " array([[5.4, 3.4, 1.5, 0.4],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.1, 3.5, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [5.9, 3. , 5.1, 1.8],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [4.8, 3.4, 1.6, 0.2]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "y, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 분류 학습\n",
    "* 입력 : 4\n",
    "* 1층 : 300\n",
    "* 2층 : 100\n",
    "* 출력 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, cost:1.1219043731689453, train accuracy:0.30000001192092896\n",
      "epoch:0, cost:1.080630898475647, train accuracy:0.30000001192092896\n",
      "epoch:0, cost:1.0594409704208374, train accuracy:0.550000011920929\n",
      "epoch:0, cost:1.053850769996643, train accuracy:0.3499999940395355\n",
      "epoch:0, cost:1.0411632061004639, train accuracy:0.6499999761581421\n",
      "epoch:0, cost:1.0214840173721313, train accuracy:0.6499999761581421\n",
      "epoch:1, cost:0.9865864515304565, train accuracy:0.699999988079071\n",
      "epoch:1, cost:0.9794443249702454, train accuracy:0.699999988079071\n",
      "epoch:1, cost:1.005603313446045, train accuracy:0.550000011920929\n",
      "epoch:1, cost:0.9603456258773804, train accuracy:0.6499999761581421\n",
      "epoch:1, cost:0.9396904110908508, train accuracy:0.6499999761581421\n",
      "epoch:1, cost:0.9357055425643921, train accuracy:0.6499999761581421\n",
      "epoch:2, cost:0.8957263827323914, train accuracy:0.699999988079071\n",
      "epoch:2, cost:0.9080840945243835, train accuracy:0.699999988079071\n",
      "epoch:2, cost:0.9456375241279602, train accuracy:0.6000000238418579\n",
      "epoch:2, cost:0.8723974227905273, train accuracy:0.6499999761581421\n",
      "epoch:2, cost:0.8565810918807983, train accuracy:0.699999988079071\n",
      "epoch:2, cost:0.8660439252853394, train accuracy:0.8999999761581421\n",
      "epoch:3, cost:0.8279536962509155, train accuracy:0.8500000238418579\n",
      "epoch:3, cost:0.848110556602478, train accuracy:0.8500000238418579\n",
      "epoch:3, cost:0.8913776278495789, train accuracy:0.6499999761581421\n",
      "epoch:3, cost:0.8104747533798218, train accuracy:0.800000011920929\n",
      "epoch:3, cost:0.7977136373519897, train accuracy:0.8999999761581421\n",
      "epoch:3, cost:0.8125964403152466, train accuracy:0.949999988079071\n",
      "epoch:4, cost:0.7789255976676941, train accuracy:0.949999988079071\n",
      "epoch:4, cost:0.8078436851501465, train accuracy:0.949999988079071\n",
      "epoch:4, cost:0.8451460003852844, train accuracy:0.8500000238418579\n",
      "epoch:4, cost:0.7587600946426392, train accuracy:1.0\n",
      "epoch:4, cost:0.750282883644104, train accuracy:1.0\n",
      "epoch:4, cost:0.7690616846084595, train accuracy:0.949999988079071\n",
      "epoch:5, cost:0.740148663520813, train accuracy:1.0\n",
      "epoch:5, cost:0.776206374168396, train accuracy:0.949999988079071\n",
      "epoch:5, cost:0.8101478815078735, train accuracy:0.949999988079071\n",
      "epoch:5, cost:0.720698893070221, train accuracy:1.0\n",
      "epoch:5, cost:0.714290976524353, train accuracy:1.0\n",
      "epoch:5, cost:0.7329314351081848, train accuracy:0.949999988079071\n",
      "epoch:6, cost:0.7063329815864563, train accuracy:1.0\n",
      "epoch:6, cost:0.7489861249923706, train accuracy:0.949999988079071\n",
      "epoch:6, cost:0.7806582450866699, train accuracy:0.949999988079071\n",
      "epoch:6, cost:0.6869887709617615, train accuracy:1.0\n",
      "epoch:6, cost:0.6827344298362732, train accuracy:1.0\n",
      "epoch:6, cost:0.7050876021385193, train accuracy:0.949999988079071\n",
      "epoch:7, cost:0.6795779466629028, train accuracy:1.0\n",
      "epoch:7, cost:0.7257145643234253, train accuracy:0.949999988079071\n",
      "epoch:7, cost:0.7562379837036133, train accuracy:0.949999988079071\n",
      "epoch:7, cost:0.6623066663742065, train accuracy:1.0\n",
      "epoch:7, cost:0.6588674783706665, train accuracy:1.0\n",
      "epoch:7, cost:0.6817464828491211, train accuracy:0.949999988079071\n",
      "epoch:8, cost:0.6569846868515015, train accuracy:1.0\n",
      "epoch:8, cost:0.7071768641471863, train accuracy:1.0\n",
      "epoch:8, cost:0.7341210246086121, train accuracy:0.949999988079071\n",
      "epoch:8, cost:0.6393772959709167, train accuracy:1.0\n",
      "epoch:8, cost:0.6373475790023804, train accuracy:1.0\n",
      "epoch:8, cost:0.6626649498939514, train accuracy:0.949999988079071\n",
      "epoch:9, cost:0.6372491121292114, train accuracy:1.0\n",
      "epoch:9, cost:0.6887471079826355, train accuracy:1.0\n",
      "epoch:9, cost:0.7152266502380371, train accuracy:0.949999988079071\n",
      "epoch:9, cost:0.6206666231155396, train accuracy:1.0\n",
      "epoch:9, cost:0.6196728348731995, train accuracy:1.0\n",
      "epoch:9, cost:0.6481917500495911, train accuracy:0.949999988079071\n",
      "epoch:10, cost:0.621559739112854, train accuracy:1.0\n",
      "epoch:10, cost:0.672751784324646, train accuracy:1.0\n",
      "epoch:10, cost:0.6991916298866272, train accuracy:0.949999988079071\n",
      "epoch:10, cost:0.6060064435005188, train accuracy:1.0\n",
      "epoch:10, cost:0.6056379079818726, train accuracy:1.0\n",
      "epoch:10, cost:0.6372908353805542, train accuracy:0.949999988079071\n",
      "epoch:11, cost:0.6090177297592163, train accuracy:1.0\n",
      "epoch:11, cost:0.6597148180007935, train accuracy:1.0\n",
      "epoch:11, cost:0.6860884428024292, train accuracy:1.0\n",
      "epoch:11, cost:0.5944005846977234, train accuracy:1.0\n",
      "epoch:11, cost:0.594659686088562, train accuracy:1.0\n",
      "epoch:11, cost:0.6293830871582031, train accuracy:0.949999988079071\n",
      "epoch:12, cost:0.5994337797164917, train accuracy:1.0\n",
      "epoch:12, cost:0.6485347747802734, train accuracy:1.0\n",
      "epoch:12, cost:0.6754034161567688, train accuracy:1.0\n",
      "epoch:12, cost:0.5855485200881958, train accuracy:1.0\n",
      "epoch:12, cost:0.5862436294555664, train accuracy:1.0\n",
      "epoch:12, cost:0.6234761476516724, train accuracy:0.949999988079071\n",
      "epoch:13, cost:0.5917837619781494, train accuracy:1.0\n",
      "epoch:13, cost:0.6394816637039185, train accuracy:1.0\n",
      "epoch:13, cost:0.6665667295455933, train accuracy:1.0\n",
      "epoch:13, cost:0.5788529515266418, train accuracy:1.0\n",
      "epoch:13, cost:0.5797589421272278, train accuracy:1.0\n",
      "epoch:13, cost:0.6191158890724182, train accuracy:0.949999988079071\n",
      "epoch:14, cost:0.5858497619628906, train accuracy:1.0\n",
      "epoch:14, cost:0.6320337057113647, train accuracy:1.0\n",
      "epoch:14, cost:0.6591686010360718, train accuracy:1.0\n",
      "epoch:14, cost:0.573706328868866, train accuracy:1.0\n",
      "epoch:14, cost:0.5747703313827515, train accuracy:1.0\n",
      "epoch:14, cost:0.6158260107040405, train accuracy:0.949999988079071\n",
      "epoch:15, cost:0.5811511278152466, train accuracy:1.0\n",
      "epoch:15, cost:0.625701367855072, train accuracy:1.0\n",
      "epoch:15, cost:0.6528835296630859, train accuracy:1.0\n",
      "epoch:15, cost:0.5697901844978333, train accuracy:1.0\n",
      "epoch:15, cost:0.5709018111228943, train accuracy:1.0\n",
      "epoch:15, cost:0.6132670640945435, train accuracy:0.949999988079071\n",
      "epoch:16, cost:0.5771692395210266, train accuracy:1.0\n",
      "epoch:16, cost:0.6206085681915283, train accuracy:1.0\n",
      "epoch:16, cost:0.647571325302124, train accuracy:1.0\n",
      "epoch:16, cost:0.5667544603347778, train accuracy:1.0\n",
      "epoch:16, cost:0.5678623914718628, train accuracy:1.0\n",
      "epoch:16, cost:0.61137855052948, train accuracy:0.949999988079071\n",
      "epoch:17, cost:0.5741521120071411, train accuracy:1.0\n",
      "epoch:17, cost:0.6160187125205994, train accuracy:1.0\n",
      "epoch:17, cost:0.6429800987243652, train accuracy:1.0\n",
      "epoch:17, cost:0.5643092393875122, train accuracy:1.0\n",
      "epoch:17, cost:0.565456748008728, train accuracy:1.0\n",
      "epoch:17, cost:0.6097234487533569, train accuracy:0.949999988079071\n",
      "epoch:18, cost:0.5712594985961914, train accuracy:1.0\n",
      "epoch:18, cost:0.6121090650558472, train accuracy:1.0\n",
      "epoch:18, cost:0.6388216018676758, train accuracy:1.0\n",
      "epoch:18, cost:0.562254786491394, train accuracy:1.0\n",
      "epoch:18, cost:0.5633012056350708, train accuracy:1.0\n",
      "epoch:18, cost:0.6085126996040344, train accuracy:0.949999988079071\n",
      "epoch:19, cost:0.5691489577293396, train accuracy:1.0\n",
      "epoch:19, cost:0.608465313911438, train accuracy:1.0\n",
      "epoch:19, cost:0.6351167559623718, train accuracy:1.0\n",
      "epoch:19, cost:0.5606332421302795, train accuracy:1.0\n",
      "epoch:19, cost:0.5616946220397949, train accuracy:1.0\n",
      "epoch:19, cost:0.6072995066642761, train accuracy:0.949999988079071\n",
      "epoch:20, cost:0.566908061504364, train accuracy:1.0\n",
      "epoch:20, cost:0.6056147217750549, train accuracy:1.0\n",
      "epoch:20, cost:0.6317310929298401, train accuracy:1.0\n",
      "epoch:20, cost:0.5593687891960144, train accuracy:1.0\n",
      "epoch:20, cost:0.5603219270706177, train accuracy:1.0\n",
      "epoch:20, cost:0.6065804362297058, train accuracy:0.949999988079071\n",
      "epoch:21, cost:0.5654911994934082, train accuracy:1.0\n",
      "epoch:21, cost:0.6026968359947205, train accuracy:1.0\n",
      "epoch:21, cost:0.6286934614181519, train accuracy:1.0\n",
      "epoch:21, cost:0.5582600235939026, train accuracy:1.0\n",
      "epoch:21, cost:0.5592333078384399, train accuracy:1.0\n",
      "epoch:21, cost:0.6056470274925232, train accuracy:0.949999988079071\n",
      "epoch:22, cost:0.563671886920929, train accuracy:1.0\n",
      "epoch:22, cost:0.60051029920578, train accuracy:1.0\n",
      "epoch:22, cost:0.6258608102798462, train accuracy:1.0\n",
      "epoch:22, cost:0.5574041604995728, train accuracy:1.0\n",
      "epoch:22, cost:0.5582482814788818, train accuracy:1.0\n",
      "epoch:22, cost:0.6052700281143188, train accuracy:0.949999988079071\n",
      "epoch:23, cost:0.5628024339675903, train accuracy:1.0\n",
      "epoch:23, cost:0.5980006456375122, train accuracy:1.0\n",
      "epoch:23, cost:0.6233078241348267, train accuracy:1.0\n",
      "epoch:23, cost:0.556607723236084, train accuracy:1.0\n",
      "epoch:23, cost:0.5575178265571594, train accuracy:1.0\n",
      "epoch:23, cost:0.6043925285339355, train accuracy:0.949999988079071\n",
      "epoch:24, cost:0.5610833168029785, train accuracy:1.0\n",
      "epoch:24, cost:0.5964952111244202, train accuracy:1.0\n",
      "epoch:24, cost:0.6208537220954895, train accuracy:1.0\n",
      "epoch:24, cost:0.5560423135757446, train accuracy:1.0\n",
      "epoch:24, cost:0.5567569136619568, train accuracy:1.0\n",
      "epoch:24, cost:0.604476809501648, train accuracy:0.949999988079071\n",
      "epoch:25, cost:0.5609970092773438, train accuracy:1.0\n",
      "epoch:25, cost:0.594061017036438, train accuracy:1.0\n",
      "epoch:25, cost:0.6186634302139282, train accuracy:1.0\n",
      "epoch:25, cost:0.5554245114326477, train accuracy:1.0\n",
      "epoch:25, cost:0.5563753843307495, train accuracy:1.0\n",
      "epoch:25, cost:0.6032339334487915, train accuracy:0.949999988079071\n",
      "epoch:26, cost:0.5587965846061707, train accuracy:1.0\n",
      "epoch:26, cost:0.5938438177108765, train accuracy:1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26, cost:0.6164405345916748, train accuracy:1.0\n",
      "epoch:26, cost:0.5551460385322571, train accuracy:1.0\n",
      "epoch:26, cost:0.555733323097229, train accuracy:1.0\n",
      "epoch:26, cost:0.604439914226532, train accuracy:0.949999988079071\n",
      "epoch:27, cost:0.560742974281311, train accuracy:1.0\n",
      "epoch:27, cost:0.591437816619873, train accuracy:1.0\n",
      "epoch:27, cost:0.614600419998169, train accuracy:1.0\n",
      "epoch:27, cost:0.5546335577964783, train accuracy:1.0\n",
      "epoch:27, cost:0.5563026666641235, train accuracy:1.0\n",
      "epoch:27, cost:0.6015103459358215, train accuracy:0.949999988079071\n",
      "epoch:28, cost:0.5569159388542175, train accuracy:1.0\n",
      "epoch:28, cost:0.5979923605918884, train accuracy:1.0\n",
      "epoch:28, cost:0.6124786734580994, train accuracy:1.0\n",
      "epoch:28, cost:0.5550807118415833, train accuracy:1.0\n",
      "epoch:28, cost:0.5567058324813843, train accuracy:1.0\n",
      "epoch:28, cost:0.6071926355361938, train accuracy:0.949999988079071\n",
      "epoch:29, cost:0.5699975490570068, train accuracy:1.0\n",
      "epoch:29, cost:0.6017049551010132, train accuracy:0.949999988079071\n",
      "epoch:29, cost:0.6106715202331543, train accuracy:1.0\n",
      "epoch:29, cost:0.5550993084907532, train accuracy:1.0\n",
      "epoch:29, cost:0.5662246942520142, train accuracy:1.0\n",
      "epoch:29, cost:0.6013793349266052, train accuracy:0.949999988079071\n",
      "Test Accuracy:0.8666666746139526\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']], dtype=np.float32)\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "y_train = np.eye(n_output)[y_train]\n",
    "y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "y_train, y_test\n",
    "\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]))\n",
    "b1 = tf.Variable(tf.zeros([n_L1]))\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]))\n",
    "b2 = tf.Variable(tf.zeros([n_L2]))\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]))\n",
    "b3 = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "def net(x):\n",
    "    L1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    hypethesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)    \n",
    "    return hypethesis\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = net(X_batch)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([dW1, db1, dW2, db2, dW3, db3], [W1,b1, W2, b2, W3, b3]))\n",
    "            \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "        print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}\")\n",
    "                                      \n",
    "is_correct = tf.equal(tf.argmax(net(X_test), axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out\n",
    "* Overfitting 방지 기술\n",
    "* 학습하는 동안 무작위로 노드를 학습에 제외\n",
    "* Tensorflow\n",
    "    * `tf.nn.dropout(layer, rate)`\n",
    "    * `rate = 0.1 ~1`\n",
    "![image.png](https://i.imgur.com/KtGFdEL.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 붗꽃 분류 Drop out 기법 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, cost:1.1284806728363037, train accuracy:0.3499999940395355\n",
      "epoch:0, cost:1.059843897819519, train accuracy:0.4000000059604645\n",
      "epoch:0, cost:1.0568817853927612, train accuracy:0.550000011920929\n",
      "epoch:0, cost:1.051559567451477, train accuracy:0.5\n",
      "epoch:0, cost:1.0036418437957764, train accuracy:0.6000000238418579\n",
      "epoch:0, cost:0.9950841665267944, train accuracy:0.6499999761581421\n",
      "epoch:1, cost:0.9870622754096985, train accuracy:0.699999988079071\n",
      "epoch:1, cost:1.0053064823150635, train accuracy:0.6000000238418579\n",
      "epoch:1, cost:0.9340278506278992, train accuracy:0.75\n",
      "epoch:1, cost:1.0025469064712524, train accuracy:0.5\n",
      "epoch:1, cost:0.9081225395202637, train accuracy:0.8500000238418579\n",
      "epoch:1, cost:0.9732887148857117, train accuracy:0.550000011920929\n",
      "epoch:2, cost:0.8855727910995483, train accuracy:0.75\n",
      "epoch:2, cost:0.9418598413467407, train accuracy:0.6000000238418579\n",
      "epoch:2, cost:0.8789995312690735, train accuracy:0.800000011920929\n",
      "epoch:2, cost:0.9997377395629883, train accuracy:0.5\n",
      "epoch:2, cost:0.8500859141349792, train accuracy:0.75\n",
      "epoch:2, cost:0.9266772270202637, train accuracy:0.6000000238418579\n",
      "epoch:3, cost:0.8314698934555054, train accuracy:0.699999988079071\n",
      "epoch:3, cost:0.8707100749015808, train accuracy:0.699999988079071\n",
      "epoch:3, cost:0.8071897625923157, train accuracy:0.800000011920929\n",
      "epoch:3, cost:0.9375349283218384, train accuracy:0.5\n",
      "epoch:3, cost:0.7693279385566711, train accuracy:0.800000011920929\n",
      "epoch:3, cost:0.868794322013855, train accuracy:0.6499999761581421\n",
      "epoch:4, cost:0.8113372921943665, train accuracy:0.800000011920929\n",
      "epoch:4, cost:0.8391284942626953, train accuracy:0.6000000238418579\n",
      "epoch:4, cost:0.8021509051322937, train accuracy:0.75\n",
      "epoch:4, cost:0.8948825001716614, train accuracy:0.5\n",
      "epoch:4, cost:0.7995259761810303, train accuracy:0.75\n",
      "epoch:4, cost:0.9154545068740845, train accuracy:0.5\n",
      "epoch:5, cost:0.7590229511260986, train accuracy:0.8999999761581421\n",
      "epoch:5, cost:0.8251632452011108, train accuracy:0.75\n",
      "epoch:5, cost:0.8050554990768433, train accuracy:0.6499999761581421\n",
      "epoch:5, cost:0.8173334002494812, train accuracy:0.6499999761581421\n",
      "epoch:5, cost:0.7386287450790405, train accuracy:0.949999988079071\n",
      "epoch:5, cost:0.8940795063972473, train accuracy:0.550000011920929\n",
      "epoch:6, cost:0.7688299417495728, train accuracy:0.949999988079071\n",
      "epoch:6, cost:0.7837134599685669, train accuracy:0.8999999761581421\n",
      "epoch:6, cost:0.795734167098999, train accuracy:0.75\n",
      "epoch:6, cost:0.8283411264419556, train accuracy:0.6499999761581421\n",
      "epoch:6, cost:0.7126888632774353, train accuracy:0.949999988079071\n",
      "epoch:6, cost:0.8080544471740723, train accuracy:0.800000011920929\n",
      "epoch:7, cost:0.7676989436149597, train accuracy:0.75\n",
      "epoch:7, cost:0.7573705911636353, train accuracy:0.8500000238418579\n",
      "epoch:7, cost:0.7622843980789185, train accuracy:0.8500000238418579\n",
      "epoch:7, cost:0.7801005840301514, train accuracy:0.800000011920929\n",
      "epoch:7, cost:0.6944687962532043, train accuracy:0.949999988079071\n",
      "epoch:7, cost:0.8246889114379883, train accuracy:0.75\n",
      "epoch:8, cost:0.746177077293396, train accuracy:0.8999999761581421\n",
      "epoch:8, cost:0.7352936863899231, train accuracy:0.8999999761581421\n",
      "epoch:8, cost:0.7682732343673706, train accuracy:0.8500000238418579\n",
      "epoch:8, cost:0.7702186703681946, train accuracy:0.75\n",
      "epoch:8, cost:0.6982649564743042, train accuracy:1.0\n",
      "epoch:8, cost:0.7944373488426208, train accuracy:0.800000011920929\n",
      "epoch:9, cost:0.7519493103027344, train accuracy:0.8999999761581421\n",
      "epoch:9, cost:0.7295154929161072, train accuracy:0.949999988079071\n",
      "epoch:9, cost:0.7765863537788391, train accuracy:0.75\n",
      "epoch:9, cost:0.7293350100517273, train accuracy:0.949999988079071\n",
      "epoch:9, cost:0.6882529258728027, train accuracy:0.949999988079071\n",
      "epoch:9, cost:0.8309813737869263, train accuracy:0.6000000238418579\n",
      "epoch:10, cost:0.7302809357643127, train accuracy:0.8999999761581421\n",
      "epoch:10, cost:0.7038863897323608, train accuracy:1.0\n",
      "epoch:10, cost:0.7928057909011841, train accuracy:0.699999988079071\n",
      "epoch:10, cost:0.7272104024887085, train accuracy:0.8500000238418579\n",
      "epoch:10, cost:0.6683805584907532, train accuracy:0.949999988079071\n",
      "epoch:10, cost:0.7731082439422607, train accuracy:0.8500000238418579\n",
      "epoch:11, cost:0.7337427139282227, train accuracy:0.8999999761581421\n",
      "epoch:11, cost:0.7149878740310669, train accuracy:0.949999988079071\n",
      "epoch:11, cost:0.7360703349113464, train accuracy:0.8500000238418579\n",
      "epoch:11, cost:0.6811298727989197, train accuracy:1.0\n",
      "epoch:11, cost:0.6676703691482544, train accuracy:1.0\n",
      "epoch:11, cost:0.7577430009841919, train accuracy:0.8500000238418579\n",
      "epoch:12, cost:0.7274506688117981, train accuracy:0.8999999761581421\n",
      "epoch:12, cost:0.6660999655723572, train accuracy:1.0\n",
      "epoch:12, cost:0.7337287664413452, train accuracy:0.8500000238418579\n",
      "epoch:12, cost:0.7045741081237793, train accuracy:0.949999988079071\n",
      "epoch:12, cost:0.6521679759025574, train accuracy:1.0\n",
      "epoch:12, cost:0.7817209959030151, train accuracy:0.8500000238418579\n",
      "epoch:13, cost:0.6998943090438843, train accuracy:0.8999999761581421\n",
      "epoch:13, cost:0.6613520383834839, train accuracy:1.0\n",
      "epoch:13, cost:0.7085782289505005, train accuracy:0.8999999761581421\n",
      "epoch:13, cost:0.6862096786499023, train accuracy:0.949999988079071\n",
      "epoch:13, cost:0.6463976502418518, train accuracy:1.0\n",
      "epoch:13, cost:0.7734278440475464, train accuracy:0.8500000238418579\n",
      "epoch:14, cost:0.6891406774520874, train accuracy:0.8999999761581421\n",
      "epoch:14, cost:0.6583724021911621, train accuracy:1.0\n",
      "epoch:14, cost:0.7151592373847961, train accuracy:1.0\n",
      "epoch:14, cost:0.6727297902107239, train accuracy:0.949999988079071\n",
      "epoch:14, cost:0.6521507501602173, train accuracy:1.0\n",
      "epoch:14, cost:0.7482717633247375, train accuracy:0.8999999761581421\n",
      "epoch:15, cost:0.6995700001716614, train accuracy:0.949999988079071\n",
      "epoch:15, cost:0.6526371836662292, train accuracy:1.0\n",
      "epoch:15, cost:0.7000502943992615, train accuracy:0.949999988079071\n",
      "epoch:15, cost:0.6638160943984985, train accuracy:0.949999988079071\n",
      "epoch:15, cost:0.6219102144241333, train accuracy:1.0\n",
      "epoch:15, cost:0.7283321619033813, train accuracy:0.949999988079071\n",
      "epoch:16, cost:0.698556125164032, train accuracy:0.949999988079071\n",
      "epoch:16, cost:0.6409050226211548, train accuracy:0.949999988079071\n",
      "epoch:16, cost:0.6982852816581726, train accuracy:0.8999999761581421\n",
      "epoch:16, cost:0.6589353084564209, train accuracy:0.8999999761581421\n",
      "epoch:16, cost:0.6239373087882996, train accuracy:1.0\n",
      "epoch:16, cost:0.7337082028388977, train accuracy:0.8999999761581421\n",
      "epoch:17, cost:0.701695442199707, train accuracy:0.8999999761581421\n",
      "epoch:17, cost:0.6336023807525635, train accuracy:0.949999988079071\n",
      "epoch:17, cost:0.7145804166793823, train accuracy:0.949999988079071\n",
      "epoch:17, cost:0.6508246660232544, train accuracy:0.949999988079071\n",
      "epoch:17, cost:0.6020393371582031, train accuracy:1.0\n",
      "epoch:17, cost:0.7253972291946411, train accuracy:0.949999988079071\n",
      "epoch:18, cost:0.6771332621574402, train accuracy:0.949999988079071\n",
      "epoch:18, cost:0.6197068691253662, train accuracy:1.0\n",
      "epoch:18, cost:0.708916187286377, train accuracy:0.8999999761581421\n",
      "epoch:18, cost:0.6608395576477051, train accuracy:0.949999988079071\n",
      "epoch:18, cost:0.5980975031852722, train accuracy:1.0\n",
      "epoch:18, cost:0.6943381428718567, train accuracy:0.8999999761581421\n",
      "epoch:19, cost:0.6625625491142273, train accuracy:1.0\n",
      "epoch:19, cost:0.6130868196487427, train accuracy:1.0\n",
      "epoch:19, cost:0.691236674785614, train accuracy:0.8999999761581421\n",
      "epoch:19, cost:0.6531370878219604, train accuracy:0.8999999761581421\n",
      "epoch:19, cost:0.6040526032447815, train accuracy:1.0\n",
      "epoch:19, cost:0.6779778599739075, train accuracy:0.949999988079071\n",
      "epoch:20, cost:0.6670251488685608, train accuracy:1.0\n",
      "epoch:20, cost:0.6003211736679077, train accuracy:1.0\n",
      "epoch:20, cost:0.6617013812065125, train accuracy:0.8999999761581421\n",
      "epoch:20, cost:0.6420753598213196, train accuracy:0.949999988079071\n",
      "epoch:20, cost:0.591112494468689, train accuracy:1.0\n",
      "epoch:20, cost:0.7098036408424377, train accuracy:0.949999988079071\n",
      "epoch:21, cost:0.6591814756393433, train accuracy:0.8999999761581421\n",
      "epoch:21, cost:0.599256157875061, train accuracy:1.0\n",
      "epoch:21, cost:0.7216694951057434, train accuracy:0.8500000238418579\n",
      "epoch:21, cost:0.6301364898681641, train accuracy:0.949999988079071\n",
      "epoch:21, cost:0.5916100740432739, train accuracy:1.0\n",
      "epoch:21, cost:0.6706072092056274, train accuracy:0.949999988079071\n",
      "epoch:22, cost:0.6458948850631714, train accuracy:0.949999988079071\n",
      "epoch:22, cost:0.6088989973068237, train accuracy:0.949999988079071\n",
      "epoch:22, cost:0.6655019521713257, train accuracy:0.949999988079071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22, cost:0.6010039448738098, train accuracy:1.0\n",
      "epoch:22, cost:0.5878660678863525, train accuracy:1.0\n",
      "epoch:22, cost:0.6870848536491394, train accuracy:0.949999988079071\n",
      "epoch:23, cost:0.6693393588066101, train accuracy:0.8999999761581421\n",
      "epoch:23, cost:0.5876394510269165, train accuracy:1.0\n",
      "epoch:23, cost:0.6567568778991699, train accuracy:0.8999999761581421\n",
      "epoch:23, cost:0.6232343912124634, train accuracy:0.949999988079071\n",
      "epoch:23, cost:0.5919989347457886, train accuracy:1.0\n",
      "epoch:23, cost:0.6895222663879395, train accuracy:0.949999988079071\n",
      "epoch:24, cost:0.6455914378166199, train accuracy:1.0\n",
      "epoch:24, cost:0.5889270901679993, train accuracy:1.0\n",
      "epoch:24, cost:0.6817403435707092, train accuracy:0.8999999761581421\n",
      "epoch:24, cost:0.6084109544754028, train accuracy:1.0\n",
      "epoch:24, cost:0.5810631513595581, train accuracy:1.0\n",
      "epoch:24, cost:0.6567188501358032, train accuracy:0.949999988079071\n",
      "epoch:25, cost:0.65928715467453, train accuracy:1.0\n",
      "epoch:25, cost:0.5795700550079346, train accuracy:1.0\n",
      "epoch:25, cost:0.6695639491081238, train accuracy:0.949999988079071\n",
      "epoch:25, cost:0.6302164793014526, train accuracy:0.949999988079071\n",
      "epoch:25, cost:0.5754985213279724, train accuracy:1.0\n",
      "epoch:25, cost:0.6524688601493835, train accuracy:0.949999988079071\n",
      "epoch:26, cost:0.6440449953079224, train accuracy:0.949999988079071\n",
      "epoch:26, cost:0.5842764973640442, train accuracy:1.0\n",
      "epoch:26, cost:0.7004393935203552, train accuracy:0.8500000238418579\n",
      "epoch:26, cost:0.6011161208152771, train accuracy:1.0\n",
      "epoch:26, cost:0.5854405164718628, train accuracy:1.0\n",
      "epoch:26, cost:0.6909785270690918, train accuracy:0.8500000238418579\n",
      "epoch:27, cost:0.675390362739563, train accuracy:0.8500000238418579\n",
      "epoch:27, cost:0.589499294757843, train accuracy:1.0\n",
      "epoch:27, cost:0.6784780621528625, train accuracy:0.8999999761581421\n",
      "epoch:27, cost:0.611663818359375, train accuracy:0.949999988079071\n",
      "epoch:27, cost:0.573961615562439, train accuracy:1.0\n",
      "epoch:27, cost:0.6786285042762756, train accuracy:0.949999988079071\n",
      "epoch:28, cost:0.6503155827522278, train accuracy:0.949999988079071\n",
      "epoch:28, cost:0.5821219086647034, train accuracy:1.0\n",
      "epoch:28, cost:0.6810718774795532, train accuracy:0.8500000238418579\n",
      "epoch:28, cost:0.6178158521652222, train accuracy:0.949999988079071\n",
      "epoch:28, cost:0.573030412197113, train accuracy:1.0\n",
      "epoch:28, cost:0.6698678135871887, train accuracy:0.949999988079071\n",
      "epoch:29, cost:0.6808265447616577, train accuracy:0.8500000238418579\n",
      "epoch:29, cost:0.5791171789169312, train accuracy:1.0\n",
      "epoch:29, cost:0.6683541536331177, train accuracy:0.949999988079071\n",
      "epoch:29, cost:0.5974350571632385, train accuracy:1.0\n",
      "epoch:29, cost:0.5721244215965271, train accuracy:1.0\n",
      "epoch:29, cost:0.6832031011581421, train accuracy:0.949999988079071\n",
      "Test Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']], dtype=np.float32)\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "y_train = np.eye(n_output)[y_train]\n",
    "y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "y_train, y_test\n",
    "\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]))\n",
    "b1 = tf.Variable(tf.zeros([n_L1]))\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]))\n",
    "b2 = tf.Variable(tf.zeros([n_L2]))\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]))\n",
    "b3 = tf.Variable(tf.zeros([n_output]))\n",
    "\n",
    "def model(x, traning=False):\n",
    "    L1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    if traning:\n",
    "        L2 = tf.nn.dropout(L2, 0.3) # Dropout 추가\n",
    "    hypethesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)    \n",
    "    return hypethesis\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = model(X_batch, traning=True)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "\n",
    "        dW1, db1, dW2, db2, dW3, db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([dW1, db1, dW2, db2, dW3, db3], [W1,b1, W2, b2, W3, b3]))\n",
    "            \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "        print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}\")\n",
    "                                      \n",
    "is_correct = tf.equal(tf.argmax(model(X_test, traning=False), axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Low-Level API DNN 실습 : MNIST 손글씨 인식\n",
    "#### MNIST Dataset 미리보기\n",
    "* Old Style(Deprecated)\n",
    "    * ```from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)```\n",
    "        * Download URL\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz\n",
    "            * https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
    "        * save to \"./mnist/data\"\n",
    "        * 70,000개 샘플\n",
    "            * train(55,000), validation(5,000), test(10,000)\n",
    "            * images : m x 784 \n",
    "            * labels : m x 10(one-hot encoding)\n",
    "* Keras\n",
    "    * ```from tensorflow import keras\n",
    "mnist = keras.datasets.mnist.load_data()```\n",
    "        * Download URL : https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "        * Save to : \"~/.keras/datasets/\"\n",
    "        * 70,000개 샘플\n",
    "            * train(60,000), test(10,000)\n",
    "            * images : m x 28 x 28\n",
    "            * label : m x 10(no one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2\n",
      "(60000, 28, 28) (10000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "[5 0 4 1 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "print(type(mnist), len(mnist))\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "print(X_train.shape, y_test.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(y_train[:5])\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, cost:1.647300124168396, train accuracy:0.8700000047683716, validation accuracy:0.8830000162124634\n",
      "epoch:1, cost:1.5647385120391846, train accuracy:0.949999988079071, validation accuracy:0.9067999720573425\n",
      "epoch:2, cost:1.5368064641952515, train accuracy:0.9599999785423279, validation accuracy:0.9233999848365784\n",
      "epoch:3, cost:1.5200315713882446, train accuracy:0.9599999785423279, validation accuracy:0.9269999861717224\n",
      "epoch:4, cost:1.502272129058838, train accuracy:0.9800000190734863, validation accuracy:0.9318000078201294\n",
      "epoch:5, cost:1.4964649677276611, train accuracy:0.9900000095367432, validation accuracy:0.9409999847412109\n",
      "epoch:6, cost:1.493650197982788, train accuracy:0.9900000095367432, validation accuracy:0.9440000057220459\n",
      "epoch:7, cost:1.4875773191452026, train accuracy:0.9900000095367432, validation accuracy:0.9476000070571899\n",
      "epoch:8, cost:1.4846160411834717, train accuracy:0.9900000095367432, validation accuracy:0.9498000144958496\n",
      "epoch:9, cost:1.4844201803207397, train accuracy:0.9900000095367432, validation accuracy:0.9538000226020813\n",
      "epoch:10, cost:1.4865449666976929, train accuracy:0.9900000095367432, validation accuracy:0.9545999765396118\n",
      "epoch:11, cost:1.4820457696914673, train accuracy:0.9800000190734863, validation accuracy:0.9571999907493591\n",
      "epoch:12, cost:1.482255220413208, train accuracy:0.9900000095367432, validation accuracy:0.9581999778747559\n",
      "epoch:13, cost:1.481970191001892, train accuracy:0.9900000095367432, validation accuracy:0.9602000117301941\n",
      "epoch:14, cost:1.477402925491333, train accuracy:0.9900000095367432, validation accuracy:0.9635999798774719\n",
      "epoch:15, cost:1.477079153060913, train accuracy:0.9900000095367432, validation accuracy:0.9657999873161316\n",
      "epoch:16, cost:1.4769090414047241, train accuracy:0.9900000095367432, validation accuracy:0.9661999940872192\n",
      "epoch:17, cost:1.4762115478515625, train accuracy:0.9900000095367432, validation accuracy:0.965399980545044\n",
      "epoch:18, cost:1.474328875541687, train accuracy:0.9900000095367432, validation accuracy:0.9670000076293945\n",
      "epoch:19, cost:1.4787788391113281, train accuracy:0.9900000095367432, validation accuracy:0.9682000279426575\n",
      "Test Accuracy:0.9696000218391418\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "n_input = 28*28\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 10\n",
    "\n",
    "(X_train, y_train_label), (X_test, y_test_label) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "\n",
    "y_valid_label, y_train_label = y_train_label[:5000], y_train_label[5000:]\n",
    "y_train = np.eye(n_output)[y_train_label]\n",
    "y_valid = np.eye(n_output)[y_valid_label]\n",
    "y_test = np.eye(n_output)[y_test_label]\n",
    "\n",
    "initializer =  tf.initializers.GlorotUniform() #xavier\n",
    "\n",
    "W1 = tf.Variable(initializer([n_input, n_L1]), name=\"W1\")\n",
    "b1 = tf.Variable(tf.zeros([n_L1]), name=\"b1\")\n",
    "\n",
    "W2 = tf.Variable(initializer([n_L1, n_L2]), name=\"W2\")\n",
    "b2 = tf.Variable(tf.zeros([n_L2]), name=\"b2\")\n",
    "\n",
    "W3 = tf.Variable(initializer([n_L2, n_output]), name=\"W3\")\n",
    "b3 = tf.Variable(tf.zeros([n_output]), name=\"b3\")\n",
    "\n",
    "\n",
    "def model(X, training=False):\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    if training:\n",
    "        L2 = tf.nn.dropout(L2, 0.3) # Dropout 추가\n",
    "    hyperthesis = tf.nn.softmax(tf.matmul(L2, W3) + b3)\n",
    "    return hyperthesis\n",
    "    \n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "step = 0\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        (X_batch, y_batch) = (X_train[i:i+ batch_size], y_train[i: i+batch_size])\n",
    "        with tf.GradientTape() as tape:\n",
    "            hypethesis = model(X_batch, training=True)\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=hypethesis))\n",
    "        DW1, Db1, DW2, Db2, DW3, Db3 = tape.gradient(cost, [W1, b1, W2, b2, W3, b3])\n",
    "        optimizer.apply_gradients(grads_and_vars=zip([DW1, Db1, DW2, Db2, DW3, Db3], [W1, b1, W2, b2, W3, b3]))\n",
    "    \n",
    "        is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_batch, axis=1))\n",
    "        acc_train = tf.reduce_mean(tf.cast(is_correct, tf.float32))                              \n",
    "    \n",
    "    hypethesis = model(X_valid, training=True)\n",
    "    is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_valid, axis=1))\n",
    "    acc_val = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    print(f\"epoch:{epoch}, cost:{cost}, train accuracy:{acc_train}, validation accuracy:{acc_val}\")\n",
    "\n",
    "hypethesis = model(X_test)\n",
    "is_correct = tf.equal(tf.argmax(hypethesis, axis=1), tf.argmax(y_test, axis=1))\n",
    "acc_test = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Test Accuracy:{}\".format(acc_test))\n",
    "#print(f\"W1:{W1}, b1:{b1}, W2:{W2}, b2:{b2}, W3:{W3}, b3:{b3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 모델 예측\n",
    "* 학습한 모델로 손글씨 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "tf.Tensor(\n",
      "[[9.1840404e-01 1.5329981e-04 5.5287294e-02 9.2188641e-03 2.4427930e-04\n",
      "  6.4138626e-04 9.1227290e-04 1.2639357e-02 4.7887539e-04 2.0203278e-03]], shape=(1, 10), dtype=float32) [0]\n",
      "(1, 784)\n",
      "tf.Tensor(\n",
      "[[3.2720440e-05 1.2047440e-04 3.4140400e-04 2.7527110e-05 9.9292153e-01\n",
      "  5.6297908e-04 8.5034582e-04 2.4305729e-03 1.0766281e-05 2.7017556e-03]], shape=(1, 10), dtype=float32) [4]\n",
      "(1, 784)\n",
      "tf.Tensor(\n",
      "[[0.01118687 0.00462004 0.00140172 0.02606832 0.02812317 0.7391868\n",
      "  0.03124182 0.0020732  0.0726084  0.08348965]], shape=(1, 10), dtype=float32) [5]\n",
      "(1, 784)\n",
      "tf.Tensor(\n",
      "[[0.0021633  0.0031693  0.01816938 0.5349326  0.00516811 0.0158814\n",
      "  0.00268715 0.00065525 0.4102155  0.00695794]], shape=(1, 10), dtype=float32) [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB4CAYAAADrPanmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAASFElEQVR4nO3de7Cc05rH8e/jEmGCJENUECLEkBAOygyGxCXlMpT4AzGFMCGYGhyiyEi5m+JgGAmDFIeUCQYnJDPlUopMIS5xqZxEpEhiXDJCEreImcjBM390v2+v3nb27t39Xvrt/n2qUnvt1benn36z9ur1rnctc3dERKR4Nso7ABERqY8acBGRglIDLiJSUGrARUQKSg24iEhBqQEXESkoNeAiIgXV8g24mbmZ/WBm/1Tj/a8r39/NbJO04ysy5TY9ym266sjveDNbW37cbmnHVytr9Qt5zMyBoe6+NKjbF3gA2BNYDIx39/nB7YOB/wY2dfefMg24QDrLbXDbOOAh4Fx3vz+oH4xy260NHLcO/C8Q/ad9zN3PCW4fjHJbk475NbNtgFnAHsDGlNqFy9x9blePy1vL98A7MrNelD6ofwP6AdOBWeV6SYCZ9QP+EViUdywtaB9371P+d073d5carQX+DtiWUrvwO+A/mv3bTNs14MAoYBPgX9z9R3efAhhwRK5RtZabgCnA6rwDEamFu69z9w/c/RdK7cHPlBry/vlG1rV2bMCHAwu8euxoQbleGmRmBwIHAPfmHUuLetnMvjCzmeUhE0mQmS0A1gGzgfvdfWXOIXWpHRvwPsB3Heq+A7bMIZaWYmYbA/8KXFjuyUiyRgKDKY3Tfg78Z7N/xS8adx8BbAX8LfBqzuF0qx0//LWUPqDQVsD3OcTSav6e0reb1/MOpBW5+8vl4nozuxhYQ+lE/ML8omo97r4OeNTMFpvZfHf/Y94xbUg79sAXASPMzIK6EeiEWxKOBE4qf8X/AjgY+GczuyvnuFqVUxqvlXRsCgzJO4iutGMP/L8onaC4yMzuBc4t17+UW0St4yygd/D7TOBJSlM2pQFmNpxSg7IQ2By4EfgfStPdpEFm9leU2sN5lKYRXgRsB7yZZ1zdabsG3N3Xm9kY4H7gZkr/Aca4+/p8Iys+d/82/N3M1gNr3L3jOQfpue2Ae4AdgR+A14Dj3f1PuUbVOjajNHNqCPAnSn8o/8bdP881qm60w4U864AfgSnuflUN978GuJTSB/pn7v5zyiEWlnKbHuU2XXXk92zgDkrfMIe5+0cph1iTlm/ARURaVUMnMc3sGDP7wMyWmtmkpIKSEuU3PcptepTb7NTdAy/P+f0QGA0sB94CTnP395MLr30pv+lRbtOj3GarkR74gcBSd/+ofALwMeDEZMISlN80KbfpUW4z1MgslB2Az4LflwN/2dUDyit5SddWu/u29DC/ym1N6sotKL+1cHdDuU1LdOxWaaQB7+wCgl99EGY2AZjQwOu0m0/KP7vNr3LbYzXnFpTfOim36fiks8pGGvDlwKDg9x0prc9Qxd2nAdNAf2l7qNv8Krd107GbHuU2Q42Mgb8FDDWzXcpraY+ltIKXJEP5TY9ymx7lNkN198Dd/Scz+wfgeUqXnv7e3bWeSEKU3/Qot+lRbrOV6YU8+qpUk3fc/YCePki5rUlduQXltxblk5g9ptzWpNNjtx1XIxQRaQlqwEVECkoNuIhIQakBFxEpqMKvB37KKacAcO+9lT1016+vLO3du3dpf4EVK1bEdW+//TYAd955Z1z37rvvxuVfftF2jo067bTT4vKJJ1aupB47dmwe4TS16HgEOOCAus6xSgfRhlszZsyI64477ri4vHBhaRe6sN147LHHAPj55+KsxKseuIhIQakBFxEpqEIOoQwfPjwu33TTTUD1V8+PPqpsltGrVy8Att5667juqKOOAuCBBypbNa5atSou33fffUD1sMq+++4bl+fNmwfAZ5+Fa/ZI6K67KvsYz5w5M8dImt9WW23V5e077rhjXA6P48jQoUPj8mabbQbASy9VtngNj+12sc8++wCw6667xnX9+vWLy8OGDQPg8ssvj+vOPvtsAM4444y4Lhx6bUbqgYuIFJQacBGRgirkEEp45njixIlA9bBJKJqREn6NfPTRR6t+Ahx99NFxefz48QBce+21cV30lQtg6dKlQPVQTjjzpZ2dc845APTv3z/nSJrfU089BVQPgXz44Ye/ul947K5cufJXt3dWN2DAgLg8derUhuIsopEjRwKVmSUA4bIhixaVlmcZN25cXBcNocyZMyeuO/DAA+PymjVr0gm2AeqBi4gUVKF64EceeSRQPU971qxZiTz3888/32k5Mnjw4Lh8zz33AHDBBRfEdeGc8nYTzbkFmDJlCgDnnntuXBfOv5WKk046Cajude++++55hdNSom/Ul112Wc2PefDBBwFYt25dXPfGG2/E5ahduOSSS5IIMRHqgYuIFJQacBGRgirUEEp0YuLxxx+P67Jaz/zjjz+Oy9FJ1FNPPTWT12520UlfgGeeeQaAp59+Oq675pprMo+pCKKT4NFJcUlOtIRGZyd4uxNObthjjz3i8pAhQxoPLGHqgYuIFFS3PXAz+z1wPLDS3fcq1/UH/h0YDHwMnOLu36QXZsmWW24J5H8F5CuvvAJUTnpAZdpWPX/xO8orv/WKroYFOOSQQwBYvXp1XNfdlYZZaqbcRtMHW6UH3ky5TUp4NXZnV8HmrZYe+EPAMR3qJgEvuvtQ4MXy75Ic5Tc9ym16lNuMdduAu/vLwNcdqk8EppfL04ExCcfV7pTf9Ci36VFuM1bvSczt3H0FgLuvMLMB3T0gCbvssgsATz75ZBYvt0Fff136exYOoVx66aUATJqUSKcjl/z2VHSVWniFWmdXEobzaptA0+Q2Wnjqu+++yyuEpDVNbpMSzstftmxZjpF0LvVZKGY2AZiQ9uu0I+U2XcpvepTbZNQ7C+VLMxsIUP65wTN37j7N3Q9wd201Urua8qvc1kXHbnqU24zV2wOfDYwDbi7/TOZ69ho1y5ZH4WX80RBKQnLNb62i9dTDBYEKoBC5Laimy22fPn3icjgzqivR0BZUFmcDOOGEE5ILLCHd9sDN7FHgdeAvzGy5mY2n9AGNNrMlwOjy75KMbVB+06Lcpku5zVi3PXB3P20DNx2ZcCyFk9ISsqvd/SuaNL/7779/XN5iiy0AePXVV7t8zNq1a+PywIEDgdx2Omnq3NYi2p0nmm/f0Y8//ghUXwmblWbK7fTppckw0ZK9UNmJC+Crr7761WM22aTUHEaL1XV8fGcn6POmKzFFRApKDbiISEEVajGrb7/9Fqhemztcrzdrffv2ze21s7TRRpW/8y+88EJcHjVqVE2Pjz43gI033jixuNrF5MmT43J0wjg8gR7Osz/iiCOA6qGuq666Ku0Qm040hBIeu9ESGFBZ23vu3LlxXbR2+GuvvRbXXX311anG2Sj1wEVECqpQPfBPP/0UqCxqlbewl9Pdibwiinov4Q5F4VK+CxYsqOl5Pv/887gcfXtavnx5AhG2nnAKW7SfY7hA2t577w1UTlZ2dOONNwLVJ9+iXZIALrroouSCLYDwaunwCu7bbrsNgCeeeCKuW7JkCQCjR4+O68JyZ8KduO6///7Ggq2DeuAiIgWlBlxEpKAKNYTSbPbaa6+4HA4tFFk0Fxbg2WefBarnzJ5//vk9fs5Vq1bF5e7WVI7WDo/miwNsu+22QGsOU3UUDn289NJLQOXrfi2ioZVow+TweQCOPfZYoPLZtpPjjz8+Lh900EFA9YSITz75pKbnGTRoUFx+7rnn4vLrr78OwKJFixoJs0fUAxcRKSg14CIiBWVZbQoMYGYNvdjEiROByiXcADfccENjQdVh8803B6q3dovWDY7WCm/AO/Ws0NZobkeMGAFUz6uP5m+Hc2HDIZY999wTqB5K6mye96GHHhqXo82hf/jhh7hu5513jsvRnOYvvvgirrvjjjsAeOihh2p8NxtUV26h8fx2JtoUOzyGw6/f4TBIIw4//PC4fN555wEwduzYRJ475O5Wz+PSyG0kPDbD4anomAyPs3qcddZZcXnXXXcFUpt33+mxqx64iEhBFeokZtQ7Cf/q5SHqxcybNy+uS6DnnauTTz4ZqO4BLly4EKj0LDqaP38+UH0Ct7NFgm699da4HJ30CeeWt8qmvj31zjvvALDTTjvFdRtapKoR4TzycJ55O5gwobJnxHXXXReXG+15R8KrO88888xEnrMn1AMXESkoNeAiIgVVqCGU999/H4D99tsv89fu3bt3XL7iiisAGDlyZOZxpCU68ZLGCZhwHnh0ErNdh01CUQ6iBaigOlfSuOi6AoDFixcn/vx5b0itHriISEHVsqXaIDObY2aLzWyRmV1cru9vZi+Y2ZLyz37ph9selNvUDADlN03KbbZqGUL5CZjo7u+a2ZbAO2b2AnAW8KK732xmk4BJwBXphVpZjTCci9y/f38gnVkg4VrC4frL06ZNA1LbYqk3pVxmmts2McDMhpHDsduVcP3pNITDf2lqxty2um574O6+wt3fLZe/BxYDOwAnAtPLd5sOjEkryDbTC+U2Lf+Hjt00KbcZ69FJTDMbDPwGeBPYzt1XQKmRN7MBiUe3AW+99VZcPuyww4BkN3GNrrQMr/z75ptv4vL111+f2Gt1Yi0wJK/cpiFc/Cfnk3RbkPOxm4eDDz44Lr/33ntpvlTT5Ta8Wjpc2zuag98os7ouPk1MzQ24mfUB/gD81t3X1Bq4mU0AJnR7R4n8Uusdldse+0zHbnqU2+zVNAvFzDal1HjPcPeZ5eovzWxg+faBwMrOHuvu09z9gHrXoGhTym06os05ld/0KLcZ6rYHbqU/qQ8Ai9399uCm2cA44Obyz1mdPDwVjzzySFyO5mQ3OoQS9hzuvvtuANauXRvXjR8/vqHn76HccpuG6EQzVG+vlqOmym84VzncKm1D26bVYrfddovLF154YVxOaoGsLjRVbu+77764HG7IHW2ltmzZsh4/ZziJInz+PPYEqGUI5RDgDGChmc0v111J6QN63MzGA58CJ6cTYtvZGuU2LcPM7DiU31Qot9kr1HKykfAvYDQFK1wc6eGHH47L0UI+0dKooe233z4uz5gxIy5/+eWXAJx++ulx3U8//dRo2LXKZTnZNIW7v9xyyy0AzJkzJ49Qmmo52UjYQx4zpjJpI+otr1mzpsvHR8vSQmWZ2OHDh8d14eJvaU5ZbMblZEPhQmFRb/mCCy6I62bPnt3l46Nlj8ONksNdosJll1Og5WRFRFqJGnARkYIq1GJWkXA4Y9SoUQBMnjw5rguHUMLdXjoKh4+uvPLKXz0+w2GTlhZuFhte3SolU6dOjcvhCc1osavOhv/69u0bl8NNi6dMmQLkNkTV1ObOnRuXox15br+9Mi8jyt33338f14XDtdFEh3CoJO/NzPW/SUSkoNSAi4gUVCFnobS4lpuF0kSachZKq2j2WSgFp1koIiKtRA24iEhBqQEXESkoNeAiIgWlBlxEpKDUgIuIFJQacBGRglIDLiJSUGrARUQKKuvFrFYDP5R/toptSPb9bHj1ra4pt92rN7eg/HZHua2WybGb6aX0AGb2divtg9dM76eZYklCs72fZounUc30fpopliRk9X40hCIiUlBqwEVECiqPBnxaDq+ZpmZ6P80USxKa7f00WzyNaqb300yxJCGT95P5GLiIiCRDQygiIgWVaQNuZseY2QdmttTMJmX52kkws0FmNsfMFpvZIjO7uFzf38xeMLMl5Z/9cohNuU0vNuU23fiU33q5eyb/gI2BZcAQoBfwR2BYVq+f0HsYCOxXLm8JfAgMA24BJpXrJwG/yzgu5Va5LVxuld/G/2XZAz8QWOruH7n7euAx4MQMX79h7r7C3d8tl78HFgM7UHof08t3mw6MyTg05TY9ym26lN8GZNmA7wB8Fvy+vFxXSGY2GPgN8CawnbuvgNKHCQzIOBzlNj3KbbqU3wZk2YB3tuFpIafAmFkf4A/Ab919Td7xoNymSblNl/LbgCwb8OXAoOD3HYHPM3z9RJjZppQ+pBnuPrNc/aWZDSzfPhBYmXFYym16lNt0Kb8NyLIBfwsYama7mFkvYCwwO8PXb5iZGfAAsNjdbw9umg2MK5fHAbMyDk25TY9ymy7ltxEZn609jtIZ2mXA5LzPHtcR/19T+nq3AJhf/ncc8OfAi8CS8s/+OcSm3Cq3hcut8tvYP12JKSJSULoSU0SkoNSAi4gUlBpwEZGCUgMuIlJQasBFRApKDbiISEGpARcRKSg14CIiBfX/1B4+7tWWUicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import img2data\n",
    "\n",
    "img_path = './img/0458.png'\n",
    "#img_path = './img/1369.png'\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "numbers = img2data.img2digits(image, (28,28), border=4)\n",
    "for i, n in enumerate(numbers):\n",
    "    print(n.shape)\n",
    "    Z = model(n.astype(np.float32)/255.0)\n",
    "    pred = np.argmax(Z, axis=1)\n",
    "    print(Z, pred)\n",
    "    plt.subplot(1, len(numbers), i+1)\n",
    "    plt.title(str(pred))\n",
    "    plt.imshow(n.reshape(28,28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 붗꽃 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 1.0477 - accuracy: 0.3667\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.8479 - accuracy: 0.6000\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.7001 - accuracy: 0.7000\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5816 - accuracy: 0.7917\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.4959 - accuracy: 0.9417\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.4376 - accuracy: 0.9417\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.3970 - accuracy: 0.9500\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.3565 - accuracy: 0.9500\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.3228 - accuracy: 0.9667\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.2899 - accuracy: 0.9583\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.2819 - accuracy: 0.9417\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.2569 - accuracy: 0.9250\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.2673 - accuracy: 0.8833\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.2407 - accuracy: 0.9250\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.2180 - accuracy: 0.9500\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.2037 - accuracy: 0.9583\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.1695 - accuracy: 0.9667\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.1668 - accuracy: 0.9750\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.1479 - accuracy: 0.9750\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.1384 - accuracy: 0.9750\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.1382 - accuracy: 0.9667\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 149us/sample - loss: 0.1275 - accuracy: 0.9667\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.1249 - accuracy: 0.9667\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.1208 - accuracy: 0.9750\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.1179 - accuracy: 0.9750\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.1108 - accuracy: 0.9667\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.1135 - accuracy: 0.9500\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.1159 - accuracy: 0.9667\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 0.1004 - accuracy: 0.9750\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.1032 - accuracy: 0.9417\n",
      "Test Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv('./data/dataset_61_iris.csv')\n",
    "\n",
    "y = np.array(csv.loc[:, 'class'])\n",
    "x = np.array(csv.loc[:, ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']])\n",
    "\n",
    "label, y = np.unique(y, return_inverse=True)\n",
    "r = np.arange(y.shape[0])\n",
    "np.random.shuffle(r)\n",
    "y = y[r]\n",
    "x = x[r]\n",
    "\n",
    "X_train, X_test = x[:120], x[120:]\n",
    "y_train, y_test = y[:120], y[120:]\n",
    "\n",
    "n_input = 4\n",
    "n_L1 = 300\n",
    "n_L2 = 100\n",
    "n_output = 3\n",
    "\n",
    "#y_train = np.eye(n_output)[y_train]\n",
    "#y_test = np.eye(n_output)[y_test]\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(n_L1, activation=tf.nn.relu, input_shape=(n_input,)),\n",
    "    keras.layers.Dense(n_L2, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(n_output, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30,  batch_size=20)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test Accuracy:{}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras MNIST 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 2s 36us/sample - loss: 0.2501 - accuracy: 0.9273 - val_loss: 0.1114 - val_accuracy: 0.9670\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 2s 28us/sample - loss: 0.0972 - accuracy: 0.9698 - val_loss: 0.0882 - val_accuracy: 0.9734\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0627 - accuracy: 0.9802 - val_loss: 0.0756 - val_accuracy: 0.9792\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0449 - accuracy: 0.9856 - val_loss: 0.0833 - val_accuracy: 0.9766\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0330 - accuracy: 0.9896 - val_loss: 0.0890 - val_accuracy: 0.9754\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 2s 28us/sample - loss: 0.0256 - accuracy: 0.9917 - val_loss: 0.0876 - val_accuracy: 0.9764\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0214 - accuracy: 0.9928 - val_loss: 0.0696 - val_accuracy: 0.9808\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.0745 - val_accuracy: 0.9836\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0147 - accuracy: 0.9948 - val_loss: 0.0964 - val_accuracy: 0.9778\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0140 - accuracy: 0.9953 - val_loss: 0.0887 - val_accuracy: 0.9796\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.99 - 2s 30us/sample - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0784 - val_accuracy: 0.9816\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0846 - val_accuracy: 0.9798\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 2s 31us/sample - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.1018 - val_accuracy: 0.9780\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.1057 - val_accuracy: 0.9784\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.1053 - val_accuracy: 0.9802\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.1098 - val_accuracy: 0.9802\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 1s 27us/sample - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.1024 - val_accuracy: 0.9822\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0083 - accuracy: 0.9968 - val_loss: 0.1032 - val_accuracy: 0.9804\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.1027 - val_accuracy: 0.9808\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.1201 - val_accuracy: 0.9796\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.1248 - val_accuracy: 0.9804\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.1063 - val_accuracy: 0.9822\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.1077 - val_accuracy: 0.9826\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 2s 28us/sample - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.1016 - val_accuracy: 0.9822\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0961 - val_accuracy: 0.9808\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 2s 28us/sample - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.1241 - val_accuracy: 0.9796\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1377 - val_accuracy: 0.9782\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 2s 29us/sample - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.1026 - val_accuracy: 0.9832\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 2s 30us/sample - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.1119 - val_accuracy: 0.9824\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 2s 28us/sample - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.1256 - val_accuracy: 0.9800\n",
      "Test Accuracy:0.9812999963760376\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import keras\n",
    "import numpy as np\n",
    "import img2data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model_save_path ='./trained_models/keras-mnist.h5'\n",
    "n_input = (28,28)\n",
    "n_L1 = 256\n",
    "n_L2 = 256\n",
    "n_output = 10\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "(X_train, y_train_label), (X_test, y_test_label) = keras.datasets.mnist.load_data()\n",
    "print(X_train.shape)\n",
    "#X_train = X_train.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "#X_test = X_test.astype(np.float32).reshape(-1, n_input)/255.0\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid_label, y_train_label = y_train_label[:5000], y_train_label[5000:]\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=n_input),\n",
    "    keras.layers.Dense(n_L1, activation=tf.nn.relu, input_shape=n_input),\n",
    "    keras.layers.Dense(n_L2, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(n_output, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_label, \n",
    "          epochs=n_epochs,  batch_size=batch_size,\n",
    "          validation_data=(X_valid, y_valid_label))\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_label, verbose=0)\n",
    "\n",
    "print(\"Test Accuracy:{}\".format(test_acc))\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACnCAYAAADqiRxlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQrElEQVR4nO3db6wc1X3G8e9TSEibRAXia2TZpiatX0ClxkFXhIi+IKFJAVU1lUIFqooVWXJfEIlIkSrTSk37Ln3RECG1qK6CcKQUQpUgLIRKLIcq6osA1wkBE8fhJnXh1hZ2CiFIkdKa/vpiz8JwvXv37+zMOfN8pNXunp1795zZM8+ePTuzo4jAzMzK8itNV8DMzObP4W5mViCHu5lZgRzuZmYFcribmRXI4W5mVqDawl3SjZJOSFqVtL+u5zEzs/Opjv3cJV0A/Aj4BLAGPAPcHhE/mPuTmZnZeeoauV8DrEbETyLif4CHgN01PZeZma1zYU3/dyvwcuX+GvCRYQtv2rQpduzYUVNVzMzKdPTo0Z9GxNKgx+oKdw0oe8f8j6R9wD6Ayy+/nJWVlZqqYmZWJkn/OeyxuqZl1oDtlfvbgFPVBSLiQEQsR8Ty0tLANx4zM5tSXeH+DLBT0hWS3g3cBhyq6bnMzGydWqZlIuKcpM8ATwAXAPdHxAt1PJeZmZ2vrjl3IuJx4PG6/r+ZmQ3nI1TNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK1BtPz9gZtYk6Z2/PF7HWefazOFuZkVZH+qDyrsQ9J6WMbNiDAv2QcuNu2yuHO5mVqxRI/SSQ97TMmZWhGFz7OsDflCY98tKmq7xyN3MOqWkAN+IR+5mlr1J94ypPl7925JG8B65m1lRJg3miBg4dZP7XLzD3cxsiJwD3uFuZsbgEXzOHO5mLdWfGsh59JijcfauycFMX6hKOgm8AbwJnIuIZUmXAl8DdgAngT+OiNdmq6aZ2WB1hG9EnPdFa26j+nmM3D8WEbsiYjnd3w8ciYidwJF038ysdvMM4I1G8Dl8oqpjWmY3cDDdPgjcUsNzmJnVLue9aGYN9wC+KemopH2p7LKIOA2QrjcP+kNJ+yStSFo5e/bsjNUwM7OqWQ9iui4iTknaDByW9MNx/zAiDgAHAJaXl/OazDKzzlg//56LmcI9Ik6l6zOSHgGuAV6RtCUiTkvaApyZQz2to7r2M62zmFcAeT2fr79Ocgr5qadlJL1X0vv7t4FPAseAQ8CetNge4NFZK2nWNetDpLpb5LBLXc9teZpl5H4Z8EjqCBcC/xwR/yrpGeBhSXuBl4BbZ6+mWZ67o03KwdpuOU3RTB3uEfET4EMDyv8buGGWSpmBT5M2Ka8fq+rMr0KO+27rDcTayP3SJlV8uE/6EWrY8t64FiuXj77zNuynaM0m5d+WGZM3NDPLKQeKHrlPMmc7zovm3fIWI6cNyKytig33Wc7MMup/Vcsc8vPlYO/xemi/tm/7RYb7vPey2OgAhnFG8x7xTyen3c7q5D5j0ygy3Kvq+JW4YYEzKMQdTuPzujKbn+K+UF1EQIxzxpZhRw06wMbj0aq1TW7bbnHhXlV3QEx7Wq7cOskieJ28zeuifXI8oK6oaZmm5ranOS1XFw6lt9m5j9i0ihm5t2m00x/Rl3bC3bqMOypq02ts7VRHH8lx1A4FhXtV21b+sKB3WJ1v/Tpq22tp7VfndpVTfywy3C0ffoOzWQ0K3Hn1q5z7ZxHhntMLkNM7/6J53di05h3wg/Z2y61/FhHuVbm9AGY2H8MCfpKQH7Z8jrlSXLhbPnL6xGV58JfxbytqV8gcdXWXyNw/8lp7DfvZilG7SpfWJx3uDfBvprxT7huRtc+o38Uftf2V0Cc9LdOQEjqPWQ66uq155N4CXZuamXUvhi6tK5uPSY8xmfWTdRv6qEfuLdHVaZpxNoI2bChWjkVsa5PupVOHkeEu6X5JZyQdq5RdKumwpBfT9SWpXJLulbQq6TlJV9dZ+dw5tGwR+kEz6tIFG50jubSfDBln5P4AcOO6sv3AkYjYCRxJ9wFuAnamyz7gvvlUsxu6sIF1oY1t0LXQHsew/dcHhfn6sJ/kMuo5F2VkuEfEt4FX1xXvBg6m2weBWyrlX4me7wAXS9oyr8pa3krb1ayNZgn0kt8MBvW9uvpfWwJ+2jn3yyLiNEC63pzKtwIvV5ZbS2XnkbRP0oqklbNnz05Zjfx1NeC62u46jTvlMGoKorSAL60945r3F6qD1uLArTgiDkTEckQsLy0tzbkaZt2x0SHz035hXUogLnLEvv55NqrHIkwb7q/0p1vS9ZlUvgZsryy3DTg1ffWsFKWERdsMC69JA6zEgG96GrDpT6fThvshYE+6vQd4tFJ+R9pr5lrg9f70jVlf052+FPMOX78uZRl5EJOkB4HrgU2S1oDPA18AHpa0F3gJuDUt/jhwM7AK/AL4dA11tsw0dfrDktX1y4X9/9H///3r3F63pkftbTAy3CPi9iEP3TBg2QDunLVSVo7cP9q3URPBldORwW0N9kWvQx+hagvTlo0sV4s8gUQpr1XT7Wjy+R3uVhuP2utVd3C0YY8Pm57DvWFd2WCaHkHlrqmpBr9u+XK4t0hJG1JX3rSa0GQ/8euaD4e7zV1bv9DKVdOB6tcvTw53q5WDYTZ+o7RpOdxtrpoeZZZgWIA72G0SDnebm0WNMv0GYjaaw93mou7A7fqotevtt8k53BtUygi0qV/eK5nXn83K4d4SuW7M/sLPrJ1G/raM2TClfPJoK79R2iyKGLlXN4JcAif3X0osaSqm5NPLdV2XX1eP3G1ipWwwpbTDbBCHewNyDZWNztHZNrmuY5tdRLzj9W/y54qb7IcO9wXL9QvIeZ4cYh4d3uFtG2lDwDe9rTvcbUOzjtbbHsK5vLlaXtrQ7x3uDWp7sGzUQdvQeUdp+/rNRQ6v9SBNjd6bHrH3FRnubT0lWK4bSR2meX1y38OoBLmt90EB3y+vQ5u28SLDPQe5bSSz6lp7rT3WB3xd6jpp+bSKCfdFvYDTKmHUmWu9zeoewbdlKqZq5EFMku6XdEbSsUrZX0v6L0nPpsvNlcfulrQq6YSk36+r4jlp85vORvoHJuV8gJLNJte+O8igPjzLAWz9v21jsMN4R6g+ANw4oPyeiNiVLo8DSLoKuA347fQ3/yDpgnlVdpS2rNSqtr7wZqOU2HeHtWGSkM/liOaR4R4R3wZeHfP/7QYeiohfRsR/AKvANTPUL2s5dACzQUoM9r6NPolWR+PDLtP83ybM8tsyn5H0XJq2uSSVbQVeriyzlsrOI2mfpBVJK2fPnp2hGoM1Hawl/faKNWecUKnj+apK7bfz2CbbPG05bbjfB/wmsAs4DfxdKh/UAwe2OiIORMRyRCwvLS1NWY12avqNxcpUd7/qar+dNpzbGOhVU+0tExGv9G9L+ifgsXR3DdheWXQbcGrq2s2oDYccQ/s7geVjnn16VJh3rd+W1t6pRu6StlTu/hHQ35PmEHCbpIskXQHsBJ6erYqTafIF8lSMzdugPlTHHh6jntPyM3LkLulB4Hpgk6Q14PPA9ZJ20ZtyOQn8GUBEvCDpYeAHwDngzoh4s56qm3XbpPtqe6TeLWrDC7q8vBwrKytz+3+L/kLIUzGLUcKBYLOoY068i+uxJJKORsTyoMeKOUK1apFHq3ZlzwJr3rzOOOY+2g1FnGZvlLqCvqt7F1jzSt3Dw+anyJE71PtbEjmdkagUXZ+SGcbrwoYpNtxh8PTMqL0ERvH8upnloOhwh8nm36eZZnGwm1kbFR/u8HYAz3OO3KFuZm3WiXDv2yiQJwl+B7uZtV2nwn0jDmwzK0kndoU0M+sah7tlx8cXmI3mcLcseNrMbDIOdzOzAjnczcwK5HA3MyuQw93MrEAOd8tG/0tVf7lqNprD3bLiYDcbj8PdzKxADnczswI53M3MCuRwNzMr0Mhwl7Rd0pOSjkt6QdJdqfxSSYclvZiuL0nlknSvpFVJz0m6uu5GmJnZO40zcj8HfC4irgSuBe6UdBWwHzgSETuBI+k+wE3AznTZB9w391qbmdmGRoZ7RJyOiO+m228Ax4GtwG7gYFrsIHBLur0b+Er0fAe4WNKWudfczMyGmmjOXdIO4MPAU8BlEXEaem8AwOa02Fbg5cqfraWy9f9rn6QVSStnz56dvOZmZjbU2OEu6X3A14HPRsTPN1p0QNl5R55ExIGIWI6I5aWlpXGrYWZmYxgr3CW9i16wfzUivpGKX+lPt6TrM6l8Ddhe+fNtwKn5VNfMzMYxzt4yAr4MHI+IL1YeOgTsSbf3AI9Wyu9Ie81cC7zen74xM7PFGOcE2dcBfwo8L+nZVPYXwBeAhyXtBV4Cbk2PPQ7cDKwCvwA+Pdcam5nZSCPDPSL+ncHz6AA3DFg+gDtnrJeZmc3AR6iamRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgdSGEw5LegM40XQ9FmgT8NOmK7FAbm+5utRWaF97fyMiBv441zhHqC7CiYhYbroSiyJpxe0tV5fa26W2Ql7t9bSMmVmBHO5mZgVqS7gfaLoCC+b2lq1L7e1SWyGj9rbiC1UzM5uvtozczcxsjhoPd0k3SjohaVXS/qbrMw+S7pd0RtKxStmlkg5LejFdX5LKJene1P7nJF3dXM0nJ2m7pCclHZf0gqS7Unmp7X2PpKclfT+1929S+RWSnkrt/Zqkd6fyi9L91fT4jibrPw1JF0j6nqTH0v2S23pS0vOSnpW0ksqy7MuNhrukC4C/B24CrgJul3RVk3WakweAG9eV7QeORMRO4Ei6D72270yXfcB9C6rjvJwDPhcRVwLXAnem17DU9v4S+HhEfAjYBdyYzjj2t8A9qb2vAXvT8nuB1yLit4B70nK5uQs4XrlfclsBPhYRuyq7PObZlyOisQvwUeCJyv27gbubrNMc27YDOFa5fwLYkm5vobdvP8A/ArcPWi7HC73TLX6iC+0Ffg34LvARege2XJjK3+rXwBPAR9PtC9NyarruE7RxG71A+zjwGL0T9xTZ1lTvk8CmdWVZ9uWmp2W2Ai9X7q+lshJdFulcsul6cyovZh2kj+EfBp6i4PamaYpn6Z0U/jDwY+BnEXEuLVJt01vtTY+/DnxgsTWeyZeAPwf+L93/AOW2FSCAb0o6KmlfKsuyLzd9hOqg0/d1bfedItaBpPcBXwc+GxE/751XffCiA8qyam9EvAnsknQx8Ahw5aDF0nW27ZX0B8CZiDgq6fp+8YBFs29rxXURcUrSZuCwpB9usGyr29v0yH0N2F65vw041VBd6vaKpC0A6fpMKs9+HUh6F71g/2pEfCMVF9vevoj4GfBv9L5ruFhSf7BUbdNb7U2P/zrw6mJrOrXrgD+UdBJ4iN7UzJcos60ARMSpdH2G3hv3NWTal5sO92eAnenb93cDtwGHGq5TXQ4Be9LtPfTmpvvld6Rv3q8FXu9/BMyBekP0LwPHI+KLlYdKbe9SGrEj6VeB36P3ZeOTwKfSYuvb218PnwK+FWmCtu0i4u6I2BYRO+htm9+KiD+hwLYCSHqvpPf3bwOfBI6Ra19uetIfuBn4Eb15y79suj5zatODwGngf+m9u++lN/d4BHgxXV+alhW9PYZ+DDwPLDdd/wnb+rv0Poo+BzybLjcX3N7fAb6X2nsM+KtU/kHgaWAV+BfgolT+nnR/NT3+wabbMGW7rwceK7mtqV3fT5cX+nmUa1/2EapmZgVqelrGzMxq4HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAv0/N5j6PVIexSgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]] [0 4 5 8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import img2data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_save_path ='./trained_models/keras-mnist.h5'\n",
    "img_path = './img/0458.png'\n",
    "#img_path = './img/1369.png'\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = keras.models.load_model(model_save_path)\n",
    "model.summary()\n",
    "\n",
    "numbers = img2data.img2digits(image, (28,28), reshape=False, border=4)\n",
    "numbers = np.array(numbers)\n",
    "Z = model.predict(numbers)\n",
    "pred = np.argmax(Z, axis=1)\n",
    "print(Z, pred)\n",
    "\n",
    "for i, (n, p) in enumerate(zip(numbers, pred)):\n",
    "    plt.subplot(1, len(numbers), i+1)\n",
    "    plt.title(str(p))\n",
    "    plt.imshow(n.reshape(28,28), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 영화리뷰 이진 분류 예제\n",
    "* IMDB Dataset\n",
    "    * IMDB : Internet Movie DataBase\n",
    "    * train 25천개, test : 25천개\n",
    "    * 각각 긍정 부정 50%씩\n",
    "    * 리뷰 데이타는 단어를 숫자 시퀀스로 이미 변환\n",
    "    * 용량 17MB\n",
    "    * keras.datasets.imdb\n",
    "        * load_data(num_words=100000) : 빈도 높은 1만 단어만 사용\n",
    "        * data : 리뷰에 포함된 단어의 인덱스 값들, max:9999\n",
    "            * 각 샘플은 python list 타입, 길이가 제 각각이라서\n",
    "        * label : 0-부정, 1-긍정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 세트와 검증(validation)셋트로 나눠서 fit()함수 호출 이때 `validation_data` 인자 사용\n",
    "    * `history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))`\n",
    "    * 이렇게 하면 `history` 디셔넉리가 'acc', 'loss', 'val_acc', 'val_loss' 4개의 키로 된 결과를 갖는다.\n",
    "        *  훈련 정확도와 손실, 검증 정확도와 손실 값이다.\n",
    "    * plot으로 각 에포크당 훈련결과와 검증 결과를 비교할 수 있다.\n",
    "        * ```\n",
    "        plt.plot(epochs, loss, 'bo', label='training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
    "        ```\n",
    "    * 이걸 보면 어느 에폭에서 과대 적합이 시작되었는지 알 수 있다. 그 에폭과 전체 훈련 셋으로 다시 훈련 시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index는 단어와 정수 인덱스를 매핑한 딕셔너리입니다\n",
    "word_index = imdb.get_word_index()\n",
    "# 정수 인덱스와 단어를 매핑하도록 뒤집습니다\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# 리뷰를 디코딩합니다. \n",
    "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위한 인덱스이므로 3을 뺍니다\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # 크기가 (len(sequences), dimension))이고 모든 원소가 0인 행렬을 만듭니다\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # results[i]에서 특정 인덱스의 위치를 1로 만듭니다\n",
    "    return results\n",
    "\n",
    "# 훈련 데이터를 벡터로 변환합니다\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# 테스트 데이터를 벡터로 변환합니다\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 벡터로 바꿉니다\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
