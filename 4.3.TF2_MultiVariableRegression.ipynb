{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Variable Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Variable Linear Regression\n",
    "* 입력 변수가 1개가 아닌 여러개인 경우 다변수 선형회귀\n",
    "    * bias 도 입력 변수가 1인 것으로 생각할 수 있다\n",
    "    * $y = W_0X_0 + W_1X_1 + W_2X_2$\n",
    "        * $W_0$ : 상수, bias\n",
    "        * $X_0$ : 1\n",
    "        * $W_n$ : n번째 계수\n",
    "        * $X_n$ : n번째 입력 변수\n",
    "![image.png](https://i.imgur.com/ttVB6jg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬곱\n",
    "* 각각의 입력 변수에 대해서 미분을 하기 불편하므로 행렬 연산을 한다.\n",
    "* 행렬곱 연산은 각각의 계수를 개별적으로 찾는 것을 쉽게 해준다.\n",
    "* 이 때 전치행렬(Transpose)가 쓸모 있다.\n",
    "![image.png](https://i.imgur.com/netr7gv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬곱을 이용한 연산\n",
    "* $H(X) = WX + b$\n",
    "* $\\begin{bmatrix} w1 & w2 & w3 \\end{bmatrix} \\times \\begin{bmatrix} x1 \\\\ x2 \\\\x3\\end{bmatrix} = \n",
    "\\begin{bmatrix} w1 \\times x1 + w2 \\times x2 + w3 \\times x3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bias 제거 단순화\n",
    "* $H(X) = WX$\n",
    "*  $\\begin{bmatrix} b & w1 & w2 & w3 \\end{bmatrix} \\times \\begin{bmatrix} 1\\\\ x1 \\\\ x2 \\\\x3\\end{bmatrix} = \n",
    "\\begin{bmatrix}b \\times 1 +  w1 \\times x1 + w2 \\times x2 + w3 \\times x3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전치 행렬\n",
    "* Transpose\n",
    "![image.png](https://i.imgur.com/0MClWDZ.png)\n",
    "\n",
    "### 전치행렬을 이용한 연산\n",
    "* $W = \\begin{bmatrix} b\\\\w1\\\\w2\\\\w3\\end{bmatrix}, x = \\begin{bmatrix} 1 \\\\x1\\\\x2\\\\x3\\end{bmatrix}$ 일때\n",
    "* $H(X) = W^TX$\n",
    "* $ =  \\begin{bmatrix}b & w1 & w2 & w3\\end{bmatrix} \n",
    " \\times \\begin{bmatrix} 1 \\\\ x1 \\\\ x2 \\\\ x3 \\end{bmatrix} = \n",
    "\\begin{bmatrix}b \\times1 + w1 \\times x1 + w2 \\times x2 + w3 \\times x3 \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2과목 점수 계산 예제\n",
    "* 2과목의 시험 점수로 학점을 계산하는 예제를 만들어 보자.\n",
    "    * 3개의 변수를 사용한다.\n",
    "        * $x_0$ = 1, bias, 50점 기본점수\n",
    "        * $x_1$ = 0~50, 가중치 0.7\n",
    "        * $x_2$ = 0~50, 가중치 0.3\n",
    "        * $y = 50x_0 + 0.7x_1 + 0.3x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  10.  53.7]\n",
      " [ 3.  40.  64.1]\n",
      " [45.  29.  90.2]\n",
      " [14.  27.  67.9]\n",
      " [ 3.  47.  66.2]]\n"
     ]
    }
   ],
   "source": [
    "m = 50\n",
    "TRUE_W1, TRUE_W2, TRUE_b = 0.7, 0.3, 50\n",
    "\n",
    "X1 = np.random.randint(0, 50, (m,1)).astype(np.float32)\n",
    "X2 = np.random.randint(0, 50, (m,1)).astype(np.float32)\n",
    "\n",
    "Y = TRUE_W1 * X1 + TRUE_W2 * X2 + TRUE_b\n",
    "\n",
    "print(np.hstack((X1,X2, Y))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개별 변수로 구현\n",
    "* 개별 변수 선언\n",
    "\n",
    "```\n",
    "x1 = x[:,0]\n",
    "x2 = x[:,1]\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([1]))\n",
    "W2 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "```\n",
    "\n",
    "* ` hypothesis = W1 * x1 + W2 * x2 + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.6335303], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-2.1107354], dtype=float32)> <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([-1.0693436], dtype=float32)>\n",
      "epoch:0, cost:11842.27734375\n",
      "epoch:2000, cost:222.41094970703125\n",
      "epoch:4000, cost:114.44172668457031\n",
      "epoch:6000, cost:58.88628387451172\n",
      "epoch:8000, cost:30.30010223388672\n",
      "epoch:10000, cost:15.590920448303223\n",
      "epoch:12000, cost:8.022322654724121\n",
      "epoch:14000, cost:4.1278910636901855\n",
      "epoch:16000, cost:2.124007225036621\n",
      "epoch:18000, cost:1.0929113626480103\n",
      "epoch:20000, cost:0.5623711347579956\n",
      "epoch:22000, cost:0.28935620188713074\n",
      "epoch:24000, cost:0.14890198409557343\n",
      "epoch:26000, cost:0.07661141455173492\n",
      "epoch:28000, cost:0.03943201154470444\n",
      "epoch:30000, cost:0.020295336842536926\n",
      "epoch:32000, cost:0.010451162233948708\n",
      "epoch:34000, cost:0.005382392555475235\n",
      "epoch:36000, cost:0.0027645432855933905\n",
      "epoch:38000, cost:0.0014364939415827394\n",
      "epoch:40000, cost:0.0007445309893228114\n",
      "Final W1:[0.70120656], W2:[0.30111507], b:[49.933083]\n"
     ]
    }
   ],
   "source": [
    "W1 = tf.Variable(tf.random.normal((1,)))\n",
    "W2 = tf.Variable(tf.random.normal((1,)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "print(W1, W2, b)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epochs = 40001\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = W1 * X1 + W2 * X2 + b\n",
    "        cost = tf.reduce_mean(tf.square(h-Y))\n",
    "    gW1, gW2, gb = tape.gradient(cost, [W1, W2, b])\n",
    "    \n",
    "    W1.assign_sub(learning_rate * gW1)\n",
    "    W2.assign_sub(learning_rate * gW2)\n",
    "    b.assign_sub(learning_rate * gb)\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f\"epoch:{epoch}, cost:{cost.numpy()}\")\n",
    "print(f\"Final W1:{W1.numpy()}, W2:{W2.numpy()}, b:{b.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix로 구현\n",
    "* `W = tf.Variable(tf.random.normal([2,1]))`\n",
    "* `hypothesis = tf.matmul(x, W) + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [2 3]\n",
      " [2 3]], shape=(3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 6  9]\n",
      " [24 36]\n",
      " [42 63]\n",
      " [60 90]], shape=(4, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(12).reshape(4,3)\n",
    "b = tf.constant([[2,3],\n",
    "                 [2,3],\n",
    "                 [2,3]])\n",
    "print(a)\n",
    "print(b)\n",
    "print(tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. 10.]\n",
      " [ 3. 40.]\n",
      " [45. 29.]\n",
      " [14. 27.]\n",
      " [ 3. 47.]]\n",
      "epoch:0, cost:23724.546875\n",
      "epoch:2000, cost:189.50196838378906\n",
      "epoch:4000, cost:116.39498138427734\n",
      "epoch:6000, cost:71.49159240722656\n",
      "epoch:8000, cost:43.911285400390625\n",
      "epoch:10000, cost:26.971010208129883\n",
      "epoch:12000, cost:16.566003799438477\n",
      "epoch:14000, cost:10.175074577331543\n",
      "epoch:16000, cost:6.249683380126953\n",
      "epoch:18000, cost:3.838665246963501\n",
      "epoch:20000, cost:2.357754707336426\n",
      "epoch:22000, cost:1.4481985569000244\n",
      "epoch:24000, cost:0.8895040154457092\n",
      "epoch:26000, cost:0.546344518661499\n",
      "epoch:28000, cost:0.33559051156044006\n",
      "epoch:30000, cost:0.2061057686805725\n",
      "epoch:32000, cost:0.12659282982349396\n",
      "epoch:34000, cost:0.07774952799081802\n",
      "epoch:36000, cost:0.04777974262833595\n",
      "epoch:38000, cost:0.02934126928448677\n",
      "epoch:40000, cost:0.018029391765594482\n",
      "Final W:[[0.7051337 ]\n",
      " [0.30807984]], b:[49.615498]\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((X1, X2))\n",
    "print(X[:5])\n",
    "\n",
    "W = tf.Variable(tf.random.normal([2,1]))\n",
    "b = tf.Variable(tf.random.normal((1, )))\n",
    "lr = 0.0005\n",
    "epochs = 40001\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(h-Y))\n",
    "    dW, db = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(lr * dW)\n",
    "    b.assign_sub(lr * db)\n",
    "    \n",
    "    if epoch % 2000 == 0:\n",
    "        print(f\"epoch:{epoch}, cost:{cost.numpy()}\")\n",
    "print(f\"Final W:{W.numpy()}, b:{b.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
